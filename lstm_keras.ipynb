{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An example how to build an LSTM language model with keras.\n",
    "* Based on: http://adventuresinmachinelearning.com/keras-lstm-tutorial/\n",
    "* We'll use a different set of parameters and data to make this managable on a CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "# import modules\n",
    "\n",
    "from __future__ import division\n",
    "\n",
    "import collections\n",
    "import os\n",
    "import numpy as np\n",
    "import argparse\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import keras\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Activation, Embedding, Dropout, TimeDistributed\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from IPython.display import SVG\n",
    "from keras_sequential_ascii import sequential_model_to_ascii_printout\n",
    "from keras.callbacks import History \n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions and classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def history_plot(history, validation=True, figsize_=(20,10)):\n",
    "    \"\"\"Plot network metric history for traning and validation sets\"\"\"\n",
    "    df = pd.DataFrame(history.history)\n",
    "    num_epochs = df.shape[0]\n",
    "    \n",
    "    # Set figure size option\n",
    "    # plt.rcParams['figure.figsize'] = figsize_\n",
    "    # Why do I need to reset this everytime?\n",
    "    plt.style.use('dark_background')\n",
    "    plt.figure(figsize=figsize_)\n",
    "    metrics_ = [x for x in df.columns.tolist() if 'val' not in x]\n",
    "    num_metrics = len(metrics_)\n",
    "    \n",
    "    for i in range(0, num_metrics):\n",
    "        plt.subplot(int(round(num_metrics/2)), 2, i+1)\n",
    "        plt.plot(df[metrics_[i]].values, 'r')\n",
    "        if validation:\n",
    "            validation_metric = 'val_' + metrics_[i]\n",
    "            plt.plot(df[validation_metric].values, 'g')\n",
    "        plt.xticks(np.arange(0, num_epochs+1, 1.0))\n",
    "        plt.xlabel(\"Num of Epochs\")\n",
    "        plt.ylabel(metrics_[i])\n",
    "        plt.title(\"Training {0} vs Validation {1}\".format(metrics_[i], metrics_[i]))\n",
    "        plt.legend(['train','validation'])\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_confusion_matrix(classifier_, x_test, y_test, figsize_=(20,10)):\n",
    "    \"\"\"Make a confusion matrix\"\"\"\n",
    "    Y_pred = classifier_.predict(x_test)\n",
    "    y_pred = np.argmax(Y_pred, axis=1)\n",
    "    \n",
    "    # Compute the confusion matrix and store as a DataFrame\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    df_cm = pd.DataFrame(cm)\n",
    "    \n",
    "    plt.figure(figsize=figsize_)\n",
    "    # Set label font size\n",
    "    sns.set(font_scale=1.4)\n",
    "    sns.heatmap(df_cm, annot=True, annot_kws={\"size\": 12})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model(model):\n",
    "    return SVG(keras.utils.vis_utils.model_to_dot(model, show_shapes=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_words(filename):\n",
    "    with tf.gfile.GFile(filename, \"r\") as f:\n",
    "        return f.read().decode(\"utf-8\").replace(\"\\n\", \"<eos>\").split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocabulary(filename):\n",
    "    \"\"\"A function to build the vocabulary from input data\"\"\"\n",
    "    data = read_words(filename)\n",
    "    \n",
    "    counter = collections.Counter(data)\n",
    "    count_pairs = sorted(counter.items(), key=lambda x: (-x[1], x[0]))\n",
    "    \n",
    "    words, _ = list(zip(*count_pairs))\n",
    "    word_to_id = dict(zip(words, range(len(words))))\n",
    "    \n",
    "    return word_to_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocabulary_news(data):\n",
    "    \n",
    "    counter = collections.Counter(data)\n",
    "    count_pairs = sorted(counter.items(), key=lambda x: (-x[1], x[0]))\n",
    "    \n",
    "    words, _ = list(zip(*count_pairs))\n",
    "    word_to_id = dict(zip(words, range(len(words))))\n",
    "    \n",
    "    return word_to_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_to_word_ids(filename, word_to_id):\n",
    "    \"\"\"A function to build the word id representation of each file\"\"\"\n",
    "    data = read_words(filename)\n",
    "    \n",
    "    return [word_to_id[word] for word in data if word in word_to_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_to_word_ids_news(data, word_to_id):\n",
    "    \"\"\"A function to build the word id representation of each file\"\"\"\n",
    "    \n",
    "    return [word_to_id[word] for word in data if word in word_to_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_path):\n",
    "    # get the data paths\n",
    "    train_path = os.path.join(data_path, \"ptb.train.txt\")\n",
    "    valid_path = os.path.join(data_path, \"ptb.valid.txt\")\n",
    "    test_path = os.path.join(data_path, \"ptb.test.txt\")\n",
    "    \n",
    "    # build the complete vocabulary and get the integer representation\n",
    "    word_to_id = build_vocabulary(train_path)\n",
    "    train_data = file_to_word_ids(train_path, word_to_id)\n",
    "    valid_data = file_to_word_ids(valid_path, word_to_id)\n",
    "    test_data = file_to_word_ids(test_path, word_to_id)\n",
    "    vocabulary = len(word_to_id)\n",
    "    reversed_dictionary = dict(zip(word_to_id.values(), word_to_id.keys()))\n",
    "    \n",
    "    return train_data, valid_data, test_data, vocabulary, reversed_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Fix up this text loading and cleaning.  We can use the functions we've already written for nlp preprocessing\n",
    "def load_data_news(train_data, valid_data, test_data):\n",
    "    train_data_ = ' '.join(train_data).decode(\"utf-8\").replace(\"\\n\", \"<eos>\").split()\n",
    "    valid_data_ = ' '.join(valid_data).decode(\"utf-8\").replace(\"\\n\", \"<eos>\").split()\n",
    "    test_data_ = ' '.join(test_data).decode(\"utf-8\").replace(\"\\n\", \"<eos>\").split()\n",
    "    \n",
    "    # build the complete vocabulary and get the integer representation\n",
    "    word_to_id = build_vocabulary_news(train_data_)\n",
    "    train_data_ = file_to_word_ids_news(train_data_, word_to_id)\n",
    "    valid_data_ = file_to_word_ids_news(valid_data_, word_to_id)\n",
    "    test_data_ = file_to_word_ids_news(test_data_, word_to_id)\n",
    "    vocabulary = len(word_to_id)\n",
    "    reversed_dictionary = dict(zip(word_to_id.values(), word_to_id.keys()))\n",
    "    \n",
    "    return train_data_, valid_data_, test_data_, vocabulary, reversed_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_text_lstm_network(vocabulary, hidden_size, input_length, num_lstms, dropout_size=None):\n",
    "    \"\"\"Function to assemble basic LSTM architecture to text sequences\"\"\"\n",
    "    # Initialize the sequential model\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Add an embedding layer. This is the first layer of the input. This takes the input words (integers at this stage), and makes a embedding vectors.\n",
    "    # We need to supply the input dimension.  In this case it's the size of the vocabulary\n",
    "    # Then we need to supply the output dimension we want.  Here, it's the size of the hidden layers (dense layers)\n",
    "    # And then we need to supply the input length, that is the number of steps/words in the sample.\n",
    "    # We'll start with the default embedding initializer, which is uniform.\n",
    "    model.add(Embedding(vocabulary, hidden_size, input_length=num_steps))\n",
    "    \n",
    "    # LSTM layers.  We need to provide the size of the hidden layers (forget gate, tanh, etc) and set return_sequences to True. This will output every output from the LSTM through \n",
    "    # the sequence as opposed to just the last output.\n",
    "    for _ in range(num_lstms):\n",
    "        model.add(LSTM(hidden_size, return_sequences=True))\n",
    "    \n",
    "    # If dropout is required\n",
    "    if dropout_size:\n",
    "        model.add(Dropout(dropout_size))\n",
    "        \n",
    "    # Now add the time distributed layer.  This adds an independent layer for each step in the sequence (or each time step, this is num_steps in this case)\n",
    "    model.add(TimeDistributed(Dense(vocabulary)))\n",
    "    \n",
    "    # The acitivation of the dense time distributed layers will be softmax\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    # Return the model object\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KerasBatchGenerator(object):\n",
    "    \"\"\"A class to generate batches of data to use in a Keras LSTM\"\"\"\n",
    "    \n",
    "    def __init__(self, data, num_steps, batch_size, vocabulary, skip_step=5):\n",
    "        self.data = data\n",
    "        # The num_steps variable is the number of words to be feed into the time distributed input layer.\n",
    "        # i.e. it is the set of words the model will learn from to predict the next words in the sequence.\n",
    "        self.num_steps = num_steps\n",
    "        self.batch_size = batch_size\n",
    "        self.vocabulary = vocabulary\n",
    "        \n",
    "        # This is the number of works to be skipped over before the next batch is taken from the data.\n",
    "        self.skip_step = skip_step\n",
    "        \n",
    "        # We need a variable to track the progess of the batches sequentially as we move through the data set.\n",
    "        # Once we reach the end of the data, we need to reset the index counter to zero to restart.\n",
    "        self.current_idx = 0\n",
    "    \n",
    "    \n",
    "    def generate(self):\n",
    "        \"\"\"Generate a batch\"\"\"\n",
    "        x = np.zeros((self.batch_size, self.num_steps))\n",
    "        y = np.zeros((self.batch_size, self.num_steps, self.vocabulary))\n",
    "        while True:\n",
    "            for i in range(self.batch_size):\n",
    "                if self.current_idx + self.num_steps >= len(self.data):\n",
    "                    # In this case, we need to reset the index\n",
    "                    self.current_idx = 0\n",
    "                # Setup the training sample: of size num_steps\n",
    "                x[i, :] = self.data[self.current_idx:self.current_idx + self.num_steps]\n",
    "                # Get the sequence for the target.  It's the next set of words shifted by 1.\n",
    "                # We'll design the model to predict the next word, so we need to increment the index by 1.\n",
    "                temp_y = self.data[self.current_idx + 1: self.current_idx + self.num_steps + 1]\n",
    "                # Convert the target sample to one-hot encoding\n",
    "                y[i, :, :] = to_categorical(temp_y, num_classes=self.vocabulary)\n",
    "                self.current_idx += self.skip_step\n",
    "            yield x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # Load the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use the Penn Tree Bank dataset to start with.  It's pretty big, so we'll see how well we can handle it on cpu.\n",
    "data_path = \"simple-examples/data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data, test_data, vocabulary, reversed_dictionary = load_data(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9970, 9971, 9972, 9974, 9975]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1132, 93, 358, 5, 329]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_data[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[102, 14, 24, 32, 752]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aer banknote berlitz calloway centrust cluett fromstein gitano guterman hydro-quebec ipo kia memotec mlx nahb punts rake regatta rubens sim snack-food ssangyong swapo wachter <eos> pierre <unk> N years old will join the board as a nonexecutive director nov. N <eos> mr. <unk> is chairman of <unk> n.v. the dutch publishing group <eos> rudolph <unk> N years old and former chairman of consolidated gold fields plc was named a nonexecutive director of this british industrial conglomerate <eos> a form of asbestos once used to make kent cigarette filters has caused a high percentage of cancer deaths among a group of\n"
     ]
    }
   ],
   "source": [
    "print(\" \".join([reversed_dictionary[x] for x in train_data[:100]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no it was n't black monday <eos> but while the\n"
     ]
    }
   ],
   "source": [
    "# what? \n",
    "print(\" \".join([reversed_dictionary[x] for x in test_data[:10]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "929589"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82430"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73760"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I might want to sample on the training data to make this more managable on my laptop..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative dataset\n",
    "* Use the 20 newsgroup dataset as an alternative. This might be a bit more managable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats = ['sci.space']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroup_train = fetch_20newsgroups(subset='train', categories=cats)\n",
    "newsgroup_test = fetch_20newsgroups(subset='test', categories=cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "593"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(newsgroup_train.filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "394"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(newsgroup_test.filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1360"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(newsgroup_train.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroup_train_ = newsgroup_train.data[0:100]\n",
    "newsgroup_valid_ = newsgroup_train.data[100:126]\n",
    "newsgroup_test_ = newsgroup_test.data[0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data, test_data, vocabulary, reversed_dictionary = load_data_news(train_data=newsgroup_train_, valid_data=newsgroup_valid_, test_data=newsgroup_test_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27008"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4534"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8378"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10228"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: henry@zoo.toronto.edu (Henry Spencer)<eos>Subject: Re: japanese moon landing?<eos>Organization: U of Toronto Zoology<eos>Lines: 21<eos><eos>In article <1qnb9tINN7ff@rave.larc.nasa.gov> C.O.EGALON@LARC.NASA.GOV (CLAUDIO OLIVEIRA EGALON) writes:<eos>>> there is no such thing as a stable lunar orbit<eos>><eos>>Is it right??? That is new stuff for me. So it means that you just can <eos>>not put a sattellite around around the Moon for too long because its <eos>>orbit will be unstable??? If so, what is the reason??? Is that because <eos>>the combined gravitacional atraction of the Sun,Moon and Earth <eos>>that does not provide a stable orbit around the Moon???<eos><eos>Any lunar satellite needs fuel to do regular orbit corrections, and\n"
     ]
    }
   ],
   "source": [
    "print(\" \".join([reversed_dictionary[x] for x in train_data[:100]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the data generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 30\n",
    "batch_size = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the training data generator class\n",
    "train_data_generator = KerasBatchGenerator(data=train_data, \n",
    "                                           num_steps=num_steps,\n",
    "                                           batch_size=batch_size,\n",
    "                                           vocabulary=vocabulary,\n",
    "                                           skip_step=num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the validation data generator class\n",
    "valid_data_generator = KerasBatchGenerator(data=valid_data,\n",
    "                                          num_steps=num_steps,\n",
    "                                          batch_size=batch_size,\n",
    "                                          vocabulary=vocabulary,\n",
    "                                          skip_step=num_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set network parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The size of the hidden layers in the LSTMs. I.e. this is the number of layers to use in the forget gate, the tanh layer, etc...\n",
    "hidden_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to be careful with the size of the network.  We'll be luck to get anything reasonable on a cpu.  \n",
    "model = make_text_lstm_network(vocabulary=vocabulary, hidden_size=hidden_size, input_length=num_steps, num_lstms=1, dropout_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 30, 10)            100000    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 30, 10)            840       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 30, 10)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 30, 10000)         110000    \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 30, 10000)         0         \n",
      "=================================================================\n",
      "Total params: 210,840\n",
      "Trainable params: 210,840\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# summarize the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           OPERATION           DATA DIMENSIONS   WEIGHTS(N)   WEIGHTS(%)\n",
      "\n",
      "               Input   #####          30\n",
      "           Embedding   emb | -------------------    100000    47.0%\n",
      "                       #####     30   10\n",
      "                LSTM   LLLLL -------------------       840     0.0%\n",
      "                tanh   #####     30   10\n",
      "             Dropout    | || -------------------         0     0.0%\n",
      "                       #####     30   10\n",
      "     TimeDistributed   ????? -------------------    110000    52.0%\n",
      "             softmax   #####     30 10000\n"
     ]
    }
   ],
   "source": [
    "# visualize the network architecture\n",
    "sequential_model_to_ascii_printout(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"450pt\" viewBox=\"0.00 0.00 502.79 450.00\" width=\"503pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 446)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"#ffffff\" points=\"-4,4 -4,-446 498.79,-446 498.79,4 -4,4\" stroke=\"transparent\"/>\n",
       "<!-- 4380728144 -->\n",
       "<g class=\"node\" id=\"node1\">\n",
       "<title>4380728144</title>\n",
       "<polygon fill=\"none\" points=\"88.6655,-324.5 88.6655,-368.5 406.1245,-368.5 406.1245,-324.5 88.6655,-324.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"170.7363\" y=\"-342.3\">embedding_1: Embedding</text>\n",
       "<polyline fill=\"none\" points=\"252.8071,-324.5 252.8071,-368.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"280.6416\" y=\"-353.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"252.8071,-346.5 308.4761,-346.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"280.6416\" y=\"-331.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"308.4761,-324.5 308.4761,-368.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"357.3003\" y=\"-353.3\">(None, 30)</text>\n",
       "<polyline fill=\"none\" points=\"308.4761,-346.5 406.1245,-346.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"357.3003\" y=\"-331.3\">(None, 30, 10)</text>\n",
       "</g>\n",
       "<!-- 4380727824 -->\n",
       "<g class=\"node\" id=\"node2\">\n",
       "<title>4380727824</title>\n",
       "<polygon fill=\"none\" points=\"121.314,-243.5 121.314,-287.5 373.4761,-287.5 373.4761,-243.5 121.314,-243.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"170.7363\" y=\"-261.3\">lstm_1: LSTM</text>\n",
       "<polyline fill=\"none\" points=\"220.1587,-243.5 220.1587,-287.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"247.9932\" y=\"-272.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"220.1587,-265.5 275.8276,-265.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"247.9932\" y=\"-250.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"275.8276,-243.5 275.8276,-287.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"324.6519\" y=\"-272.3\">(None, 30, 10)</text>\n",
       "<polyline fill=\"none\" points=\"275.8276,-265.5 373.4761,-265.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"324.6519\" y=\"-250.3\">(None, 30, 10)</text>\n",
       "</g>\n",
       "<!-- 4380728144&#45;&gt;4380727824 -->\n",
       "<g class=\"edge\" id=\"edge2\">\n",
       "<title>4380728144-&gt;4380727824</title>\n",
       "<path d=\"M247.395,-324.3664C247.395,-316.1516 247.395,-306.6579 247.395,-297.7252\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"250.8951,-297.6068 247.395,-287.6068 243.8951,-297.6069 250.8951,-297.6068\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 4380729296 -->\n",
       "<g class=\"node\" id=\"node3\">\n",
       "<title>4380729296</title>\n",
       "<polygon fill=\"none\" points=\"106.9346,-162.5 106.9346,-206.5 387.8555,-206.5 387.8555,-162.5 106.9346,-162.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"170.7363\" y=\"-180.3\">dropout_1: Dropout</text>\n",
       "<polyline fill=\"none\" points=\"234.5381,-162.5 234.5381,-206.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"262.3726\" y=\"-191.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"234.5381,-184.5 290.207,-184.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"262.3726\" y=\"-169.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"290.207,-162.5 290.207,-206.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"339.0313\" y=\"-191.3\">(None, 30, 10)</text>\n",
       "<polyline fill=\"none\" points=\"290.207,-184.5 387.8555,-184.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"339.0313\" y=\"-169.3\">(None, 30, 10)</text>\n",
       "</g>\n",
       "<!-- 4380727824&#45;&gt;4380729296 -->\n",
       "<g class=\"edge\" id=\"edge3\">\n",
       "<title>4380727824-&gt;4380729296</title>\n",
       "<path d=\"M247.395,-243.3664C247.395,-235.1516 247.395,-225.6579 247.395,-216.7252\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"250.8951,-216.6068 247.395,-206.6068 243.8951,-216.6069 250.8951,-216.6068\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 4858828496 -->\n",
       "<g class=\"node\" id=\"node4\">\n",
       "<title>4858828496</title>\n",
       "<polygon fill=\"none\" points=\"0,-81.5 0,-125.5 494.79,-125.5 494.79,-81.5 0,-81.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"160.2363\" y=\"-99.3\">time_distributed_1(dense_1): TimeDistributed(Dense)</text>\n",
       "<polyline fill=\"none\" points=\"320.4727,-81.5 320.4727,-125.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"348.3071\" y=\"-110.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"320.4727,-103.5 376.1416,-103.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"348.3071\" y=\"-88.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"376.1416,-81.5 376.1416,-125.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"435.4658\" y=\"-110.3\">(None, 30, 10)</text>\n",
       "<polyline fill=\"none\" points=\"376.1416,-103.5 494.79,-103.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"435.4658\" y=\"-88.3\">(None, 30, 10000)</text>\n",
       "</g>\n",
       "<!-- 4380729296&#45;&gt;4858828496 -->\n",
       "<g class=\"edge\" id=\"edge4\">\n",
       "<title>4380729296-&gt;4858828496</title>\n",
       "<path d=\"M247.395,-162.3664C247.395,-154.1516 247.395,-144.6579 247.395,-135.7252\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"250.8951,-135.6068 247.395,-125.6068 243.8951,-135.6069 250.8951,-135.6068\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 4858830352 -->\n",
       "<g class=\"node\" id=\"node5\">\n",
       "<title>4858830352</title>\n",
       "<polygon fill=\"none\" points=\"84.7759,-.5 84.7759,-44.5 410.0142,-44.5 410.0142,-.5 84.7759,-.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"160.2363\" y=\"-18.3\">activation_1: Activation</text>\n",
       "<polyline fill=\"none\" points=\"235.6968,-.5 235.6968,-44.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"263.5313\" y=\"-29.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"235.6968,-22.5 291.3657,-22.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"263.5313\" y=\"-7.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"291.3657,-.5 291.3657,-44.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"350.6899\" y=\"-29.3\">(None, 30, 10000)</text>\n",
       "<polyline fill=\"none\" points=\"291.3657,-22.5 410.0142,-22.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"350.6899\" y=\"-7.3\">(None, 30, 10000)</text>\n",
       "</g>\n",
       "<!-- 4858828496&#45;&gt;4858830352 -->\n",
       "<g class=\"edge\" id=\"edge5\">\n",
       "<title>4858828496-&gt;4858830352</title>\n",
       "<path d=\"M247.395,-81.3664C247.395,-73.1516 247.395,-63.6579 247.395,-54.7252\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"250.8951,-54.6068 247.395,-44.6068 243.8951,-54.6069 250.8951,-54.6068\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 4380728272 -->\n",
       "<g class=\"node\" id=\"node6\">\n",
       "<title>4380728272</title>\n",
       "<polygon fill=\"none\" points=\"204.395,-405.5 204.395,-441.5 290.395,-441.5 290.395,-405.5 204.395,-405.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"247.395\" y=\"-419.3\">4380728272</text>\n",
       "</g>\n",
       "<!-- 4380728272&#45;&gt;4380728144 -->\n",
       "<g class=\"edge\" id=\"edge1\">\n",
       "<title>4380728272-&gt;4380728144</title>\n",
       "<path d=\"M247.395,-405.2521C247.395,-397.3888 247.395,-387.9498 247.395,-378.9612\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"250.8951,-378.7376 247.395,-368.7377 243.8951,-378.7377 250.8951,-378.7376\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So this model is pretty massive.  I doubt this will train on my laptop.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointer = ModelCheckpoint(filepath='/model-{epoch:02d}.hdf5', verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before fitting, define the history object for the callbacks.\n",
    "history_lstm = History()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll need to be conservative with the number of epochs we train this on.  \n",
    "num_epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "  79/1549 [>.............................] - ETA: 18:08 - loss: 9.0747 - categorical_accuracy: 0.0423"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-11dc4bc188f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m                     \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalid_data_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m                     validation_steps=len(valid_data)//(batch_size * num_steps))\n\u001b[0m",
      "\u001b[0;32m/Users/danielokeeffe/miniconda2/envs/basic_ml/lib/python2.7/site-packages/keras/legacy/interfaces.pyc\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/danielokeeffe/miniconda2/envs/basic_ml/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/danielokeeffe/miniconda2/envs/basic_ml/lib/python2.7/site-packages/keras/engine/training_generator.pyc\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/danielokeeffe/miniconda2/envs/basic_ml/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/danielokeeffe/miniconda2/envs/basic_ml/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2719\u001b[0m                     \u001b[0;34m'In order to feed symbolic tensors to a Keras model '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2720\u001b[0m                     'in TensorFlow, you need tensorflow 1.8 or higher.')\n\u001b[0;32m-> 2721\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2722\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2723\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/danielokeeffe/miniconda2/envs/basic_ml/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m_legacy_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2691\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2692\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2693\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2694\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2695\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/danielokeeffe/miniconda2/envs/basic_ml/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/danielokeeffe/miniconda2/envs/basic_ml/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1140\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1141\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/danielokeeffe/miniconda2/envs/basic_ml/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1321\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/danielokeeffe/miniconda2/envs/basic_ml/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/danielokeeffe/miniconda2/envs/basic_ml/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1310\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1311\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1312\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/danielokeeffe/miniconda2/envs/basic_ml/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1418\u001b[0m         return tf_session.TF_Run(\n\u001b[1;32m   1419\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1420\u001b[0;31m             status, run_metadata)\n\u001b[0m\u001b[1;32m   1421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1422\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit_generator(train_data_generator.generate(), \n",
    "                    len(train_data)//(batch_size * num_steps), \n",
    "                    num_epochs, \n",
    "                    validation_data=valid_data_generator.generate(), \n",
    "                    validation_steps=len(valid_data)//(batch_size * num_steps),\n",
    "                    callbacks=[checkpointer, history_lstm])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:basic_ml]",
   "language": "python",
   "name": "conda-env-basic_ml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
