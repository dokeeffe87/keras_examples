{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec in keras. \n",
    "* based partially on: https://adventuresinmachinelearning.com/word2vec-keras-tutorial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll implement the skip-gram model first (CBOW later)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that to implement negative sampling, we'll follow the following procedure:\n",
    "* Construct a binary output layer instead of multi-class\n",
    "* Output is True if we are suppling the target word and a true context word\n",
    "* Output is False if we are suppling the target word and a a negative sample (i.e. a word sampled which is NOT in the context of the target)\n",
    "* To ensure that similar words end up having similar embeddings, we'll use cosine similarity. This should give a 1 when two words are in the same context and 0 otherwise\n",
    "Broadly, the network will look like:\n",
    "* An integer input (i.e. the index of the target word in the vocabulary) and a true or false context word\n",
    "* An embedding layer lookup (i.e. looking up the embedding vector in the embedding matrix)\n",
    "* The dot product operation (i.e. to compute the cosine similarity)\n",
    "* The output layer: a sigmoid here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules:\n",
    "from __future__ import division\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import keras\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "import functools\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from IPython.display import SVG\n",
    "from keras_sequential_ascii import sequential_model_to_ascii_printout\n",
    "from keras.callbacks import History \n",
    "import urllib\n",
    "import zipfile\n",
    "import tensorflow as tf\n",
    "import collections\n",
    "sys.path.insert(0, '/Users/danielokeeffe/Documents/src/nlp_preprocessing/')\n",
    "import nlp_preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'nlp_preprocessing' from '/Users/danielokeeffe/Documents/src/nlp_preprocessing/nlp_preprocessing.py'>"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reload nlp_preprocessing if you add any functions to it\n",
    "reload(nlp_preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimilarityCallback:\n",
    "    def __init__(self, valid_size, dictionary, valid_examples, vocab_size, validation_model):\n",
    "        self.valid_size = valid_size\n",
    "        self.dictionary = dictionary\n",
    "        self.valid_examples = valid_examples\n",
    "        self.vocab_size = vocab_size\n",
    "        self.validation_model = validation_model\n",
    "        \n",
    "    def run_sim(self):\n",
    "        for i in range(self.valid_size):\n",
    "            valid_word = self.dictionary[self.valid_examples[i]]\n",
    "            # Define the number of nearest neighbours\n",
    "            top_k = 8\n",
    "            sim = self._get_sim(self.valid_examples[i])\n",
    "            nearest = (-sim).argsort()[1:top_k+1]\n",
    "            log_str = 'Nearest to {0}: '.format(valid_word)\n",
    "            for k in range(top_k):\n",
    "                close_word = self.dictionary[nearest[k]]\n",
    "                log_str = '{0} {1},'.format(log_str, close_word)\n",
    "            print(log_str)\n",
    "            \n",
    "\n",
    "    def _get_sim(self, valid_word_idx):\n",
    "        sim = np.zeros((self.vocab_size,))\n",
    "        in_arr1 = np.zeros((1,))\n",
    "        in_arr2 = np.zeros((1,))\n",
    "        for i in range(self.vocab_size):\n",
    "            in_arr1[0,] = valid_word_idx\n",
    "            in_arr2[0,] = i\n",
    "            out = self.validation_model.predict_on_batch([in_arr1, in_arr2])\n",
    "            sim[i] = out\n",
    "        return sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model(model):\n",
    "    return SVG(keras.utils.vis_utils.model_to_dot(model, show_shapes=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_on_validation_model(word_list, dictionary, reversed_dictionary, validation_model, vocab_size, top_k=8):\n",
    "    \"\"\"A function to return the top_k most similar words after training of the primary model\"\"\"\n",
    "    # Verify that the words in the input word list are actually in the dictionary\n",
    "    try:\n",
    "        word_indicies = [reversed_dictionary[word] for word in word_list]\n",
    "    except KeyError:\n",
    "        print('a supplied word is not in dictionary')\n",
    "        return None\n",
    "    for i in range(len(word_indices)):\n",
    "        test_word = word_list[i]\n",
    "        sim = validation_get_sim(valid_word_idx=word_indices[i], \n",
    "                                 validation_model=validation_model, \n",
    "                                 vocab_size=vocab_size)\n",
    "        nearest = (-sim).argsort()[1:top_k+1]\n",
    "        log_str = 'Nearest to {0}: '.format(test_word)\n",
    "        for k in range(top_k):\n",
    "            close_word = dictionary[nearest[k]]\n",
    "            log_str = '{0} {1},'.format(log_str, close_word)\n",
    "        print(log_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_get_sim(valid_word_idx, validation_model, vocab_size):\n",
    "    sim = np.zeros((vocab_size,))\n",
    "    in_arr1 = np.zeros((1,))\n",
    "    in_arr2 = np.zeros((1,))\n",
    "    for i in range(vocab_size):\n",
    "        in_arr1[0,] = valid_word_idx\n",
    "        in_arr2[0,] = i\n",
    "        out = validation_model.predict_on_batch([in_arr1, in_arr2])\n",
    "        sim[i] = out\n",
    "    return sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data.  We'll use the text data at: 'http://mattmahoney.net/dc/' The filename is text8.zip\n",
    "url = 'http://mattmahoney.net/dc/'\n",
    "filename = download_file('text8.zip', url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = nlp_preprocessing.download_file('text8.zip', url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = nlp_preprocessing.read_data_from_zip(filename=filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17005207"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocabulary[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is pretty big, so we'll produce a trimmed version of the vocabulary of the n most common words, say 10,000\n",
    "vocab_size = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "trimmed_vocabulary = nlp_preprocessing.trim_vocabulary(vocab=vocabulary, vocab_size=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trimmed_vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trimmed_vocabulary[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'of', 'and', 'one', 'in', 'a', 'to', 'zero', 'nine', 'two']"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trimmed_vocabulary[0][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is pretty big.  We'll use it to build the dictionary and reversed dictionary, but then remove it to save memory.\n",
    "dictionary = nlp_preprocessing.create_dictionary(trimmed_vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gensim Dictionary type detected\n"
     ]
    }
   ],
   "source": [
    "# Make the reversed dictionary\n",
    "reversed_dictionary = nlp_preprocessing.create_reversed_dictionary(dictionary_=dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'bertrand'"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary[1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reversed_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reversed_dictionary['bertrand']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So now we don't need the full vocabulary anymore.\n",
    "del vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define some basic parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the window of words around the target to will be used to draw context samples:\n",
    "window_size = 3\n",
    "# The size of each word embedding vector:\n",
    "vector_dim = 300\n",
    "# The number of epochs to train for. This is going to be big.  Even with negative sampling, creating a decent embedding matrix can take awhile\n",
    "epochs = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll need to setup a validation set. This will be used to check to see what words grow in similarity as we train the network.\n",
    "# We'll select a random set of words to measure similarity on\n",
    "valid_size = 16\n",
    "# We want to select these words from the top 100 most common words in the dataset.  We didn't explicitly create the dictionary this way...we'll do it at random for now...\n",
    "valid_window = 100\n",
    "valid_examples = np.random.choice(valid_window, valid_size, replace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement skipgram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The sampling table will make sure we sample both true and false context words in a balanced way and not just select the most common words\n",
    "sampling_table = keras.preprocessing.sequence.make_sampling_table(vocab_size)\n",
    "# We need the keys of the dictionary, i.e. the index of each word in the dictionary for the skipgram sampling. \n",
    "couples, labels = keras.preprocessing.sequence.skipgrams(dictionary.keys(), vocab_size, window_size=window_size, sampling_table=sampling_table)\n",
    "word_target, word_context = zip(*couples)\n",
    "word_target = np.array(word_target, dtype='int32')\n",
    "word_context = np.array(word_context, dtype='int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([[8613, 9158], [5127, 4965], [8980, 2102], [8454, 9272], [2828, 9137], [9921, 3295], [2586, 2784], [5748, 8208], [5840, 7968], [5590, 9087]], [1, 1, 1, 1, 1, 1, 0, 1, 1, 0])\n"
     ]
    }
   ],
   "source": [
    "print(couples[:10], labels[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'renamed'"
      ]
     },
     "execution_count": 379,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary[7738]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'broadcaster'"
      ]
     },
     "execution_count": 380,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary[1218]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To actually implement the embedding layers, we'll need to use the functional API.  We need to share a single embedding layer between two tensors (target word and context \n",
    "# words), and an additional output to measure similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create input variables, one for the target, one for the context. We're just going to supply the target word and a context word individually, so the size of the inputs are both 1.\n",
    "input_target = keras.Input((1,))\n",
    "input_context = keras.Input((1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the embedding layer (we should try out different initializers for the weights also):\n",
    "embedding = keras.layers.Embedding(vocab_size, vector_dim, input_length=1, name='embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we need to be able to lookup target and context words in the embedding layer to measure similarity.\n",
    "target = embedding(input_target)\n",
    "target = keras.layers.Reshape((vector_dim, 1))(target)\n",
    "context = embedding(input_context)\n",
    "context = keras.layers.Reshape((vector_dim, 1))(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the similarity measurement\n",
    "similarity = keras.layers.dot([target, context], normalize=True, axes=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_product = keras.layers.dot([target, context], normalize=False, axes=0)\n",
    "dot_product = keras.layers.Reshape((1,))(dot_product)\n",
    "output = keras.layers.Dense(1, activation='sigmoid')(dot_product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model = keras.Model(inputs=[input_target, input_context], outputs=output)\n",
    "model.compile(loss='binary_crossentropy', optimizer='rmsprop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_8 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 1, 300)       3000000     input_7[0][0]                    \n",
      "                                                                 input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_20 (Reshape)            (None, 300, 1)       0           embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_21 (Reshape)            (None, 300, 1)       0           embedding[1][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dot_18 (Dot)                    (300, 1, 1)          0           reshape_20[0][0]                 \n",
      "                                                                 reshape_21[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "reshape_22 (Reshape)            (300, 1)             0           dot_18[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (300, 1)             2           reshape_22[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 3,000,002\n",
      "Trainable params: 3,000,002\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           OPERATION           DATA DIMENSIONS   WEIGHTS(N)   WEIGHTS(%)\n",
      "\n",
      "               Input   #####           1\n",
      "          InputLayer     |   -------------------         0     0.0%\n",
      "                       #####           1\n",
      "          InputLayer     |   -------------------         0     0.0%\n",
      "                       #####           1\n",
      "           Embedding   emb | -------------------   3000000    99.0%\n",
      "                       #####      1  300\n",
      "             Reshape     |   -------------------         0     0.0%\n",
      "                       #####    300    1\n",
      "             Reshape     |   -------------------         0     0.0%\n",
      "                       #####    300    1\n",
      "                 Dot   ????? -------------------         0     0.0%\n",
      "                       #####      1    1\n",
      "             Reshape     |   -------------------         0     0.0%\n",
      "                       #####           1\n",
      "               Dense   XXXXX -------------------         2     0.0%\n",
      "             sigmoid   #####           1\n"
     ]
    }
   ],
   "source": [
    "sequential_model_to_ascii_printout(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"458pt\" viewBox=\"0.00 0.00 601.89 458.00\" width=\"602pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 454)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"#ffffff\" points=\"-4,4 -4,-454 597.8867,-454 597.8867,4 -4,4\" stroke=\"transparent\"/>\n",
       "<!-- 5248837072 -->\n",
       "<g class=\"node\" id=\"node1\">\n",
       "<title>5248837072</title>\n",
       "<polygon fill=\"none\" points=\"34.1035,-405.5 34.1035,-449.5 287.7832,-449.5 287.7832,-405.5 34.1035,-405.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"98.2847\" y=\"-423.3\">input_7: InputLayer</text>\n",
       "<polyline fill=\"none\" points=\"162.4658,-405.5 162.4658,-449.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"190.3003\" y=\"-434.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"162.4658,-427.5 218.1348,-427.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"190.3003\" y=\"-412.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"218.1348,-405.5 218.1348,-449.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"252.959\" y=\"-434.3\">(None, 1)</text>\n",
       "<polyline fill=\"none\" points=\"218.1348,-427.5 287.7832,-427.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"252.959\" y=\"-412.3\">(None, 1)</text>\n",
       "</g>\n",
       "<!-- 5248937936 -->\n",
       "<g class=\"node\" id=\"node3\">\n",
       "<title>5248937936</title>\n",
       "<polygon fill=\"none\" points=\"145.2139,-324.5 145.2139,-368.5 448.6729,-368.5 448.6729,-324.5 145.2139,-324.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"220.2847\" y=\"-342.3\">embedding: Embedding</text>\n",
       "<polyline fill=\"none\" points=\"295.3555,-324.5 295.3555,-368.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"323.1899\" y=\"-353.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"295.3555,-346.5 351.0244,-346.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"323.1899\" y=\"-331.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"351.0244,-324.5 351.0244,-368.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"399.8486\" y=\"-353.3\">(None, 1)</text>\n",
       "<polyline fill=\"none\" points=\"351.0244,-346.5 448.6729,-346.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"399.8486\" y=\"-331.3\">(None, 1, 300)</text>\n",
       "</g>\n",
       "<!-- 5248837072&#45;&gt;5248937936 -->\n",
       "<g class=\"edge\" id=\"edge1\">\n",
       "<title>5248837072-&gt;5248937936</title>\n",
       "<path d=\"M198.106,-405.3664C214.4473,-395.6337 233.803,-384.1057 251.0351,-373.8424\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"253.0251,-376.731 259.8257,-368.6068 249.4431,-370.7169 253.0251,-376.731\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 5248837136 -->\n",
       "<g class=\"node\" id=\"node2\">\n",
       "<title>5248837136</title>\n",
       "<polygon fill=\"none\" points=\"306.1035,-405.5 306.1035,-449.5 559.7832,-449.5 559.7832,-405.5 306.1035,-405.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"370.2847\" y=\"-423.3\">input_8: InputLayer</text>\n",
       "<polyline fill=\"none\" points=\"434.4658,-405.5 434.4658,-449.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"462.3003\" y=\"-434.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"434.4658,-427.5 490.1348,-427.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"462.3003\" y=\"-412.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"490.1348,-405.5 490.1348,-449.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"524.959\" y=\"-434.3\">(None, 1)</text>\n",
       "<polyline fill=\"none\" points=\"490.1348,-427.5 559.7832,-427.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"524.959\" y=\"-412.3\">(None, 1)</text>\n",
       "</g>\n",
       "<!-- 5248837136&#45;&gt;5248937936 -->\n",
       "<g class=\"edge\" id=\"edge2\">\n",
       "<title>5248837136-&gt;5248937936</title>\n",
       "<path d=\"M395.7808,-405.3664C379.4395,-395.6337 360.0838,-384.1057 342.8516,-373.8424\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"344.4436,-370.7169 334.061,-368.6068 340.8617,-376.731 344.4436,-370.7169\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 5248933968 -->\n",
       "<g class=\"node\" id=\"node4\">\n",
       "<title>5248933968</title>\n",
       "<polygon fill=\"none\" points=\"0,-243.5 0,-287.5 287.8867,-287.5 287.8867,-243.5 0,-243.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"67.2847\" y=\"-261.3\">reshape_20: Reshape</text>\n",
       "<polyline fill=\"none\" points=\"134.5693,-243.5 134.5693,-287.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"162.4038\" y=\"-272.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"134.5693,-265.5 190.2383,-265.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"162.4038\" y=\"-250.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"190.2383,-243.5 190.2383,-287.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"239.0625\" y=\"-272.3\">(None, 1, 300)</text>\n",
       "<polyline fill=\"none\" points=\"190.2383,-265.5 287.8867,-265.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"239.0625\" y=\"-250.3\">(None, 300, 1)</text>\n",
       "</g>\n",
       "<!-- 5248937936&#45;&gt;5248933968 -->\n",
       "<g class=\"edge\" id=\"edge3\">\n",
       "<title>5248937936-&gt;5248933968</title>\n",
       "<path d=\"M255.1354,-324.3664C236.4985,-314.4998 214.3764,-302.7881 194.7911,-292.4194\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"196.1762,-289.1925 185.7007,-287.6068 192.901,-295.379 196.1762,-289.1925\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 5248936144 -->\n",
       "<g class=\"node\" id=\"node5\">\n",
       "<title>5248936144</title>\n",
       "<polygon fill=\"none\" points=\"306,-243.5 306,-287.5 593.8867,-287.5 593.8867,-243.5 306,-243.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"373.2847\" y=\"-261.3\">reshape_21: Reshape</text>\n",
       "<polyline fill=\"none\" points=\"440.5693,-243.5 440.5693,-287.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"468.4038\" y=\"-272.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"440.5693,-265.5 496.2383,-265.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"468.4038\" y=\"-250.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"496.2383,-243.5 496.2383,-287.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"545.0625\" y=\"-272.3\">(None, 1, 300)</text>\n",
       "<polyline fill=\"none\" points=\"496.2383,-265.5 593.8867,-265.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"545.0625\" y=\"-250.3\">(None, 300, 1)</text>\n",
       "</g>\n",
       "<!-- 5248937936&#45;&gt;5248936144 -->\n",
       "<g class=\"edge\" id=\"edge4\">\n",
       "<title>5248937936-&gt;5248936144</title>\n",
       "<path d=\"M338.7513,-324.3664C357.3882,-314.4998 379.5104,-302.7881 399.0956,-292.4194\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"400.9857,-295.379 408.186,-287.6068 397.7105,-289.1925 400.9857,-295.379\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 5043124368 -->\n",
       "<g class=\"node\" id=\"node6\">\n",
       "<title>5043124368</title>\n",
       "<polygon fill=\"none\" points=\"129.6587,-162.5 129.6587,-206.5 464.228,-206.5 464.228,-162.5 129.6587,-162.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"171.2983\" y=\"-180.3\">dot_18: Dot</text>\n",
       "<polyline fill=\"none\" points=\"212.938,-162.5 212.938,-206.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"240.7725\" y=\"-191.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"212.938,-184.5 268.6069,-184.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"240.7725\" y=\"-169.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"268.6069,-162.5 268.6069,-206.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"366.4175\" y=\"-191.3\">[(None, 300, 1), (None, 300, 1)]</text>\n",
       "<polyline fill=\"none\" points=\"268.6069,-184.5 464.228,-184.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"366.269\" y=\"-169.3\">(300, 1, 1)</text>\n",
       "</g>\n",
       "<!-- 5248933968&#45;&gt;5043124368 -->\n",
       "<g class=\"edge\" id=\"edge5\">\n",
       "<title>5248933968-&gt;5043124368</title>\n",
       "<path d=\"M185.7513,-243.3664C204.3882,-233.4998 226.5104,-221.7881 246.0956,-211.4194\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"247.9857,-214.379 255.186,-206.6068 244.7105,-208.1925 247.9857,-214.379\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 5248936144&#45;&gt;5043124368 -->\n",
       "<g class=\"edge\" id=\"edge6\">\n",
       "<title>5248936144-&gt;5043124368</title>\n",
       "<path d=\"M408.1354,-243.3664C389.4985,-233.4998 367.3764,-221.7881 347.7911,-211.4194\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"349.1762,-208.1925 338.7007,-206.6068 345.901,-214.379 349.1762,-208.1925\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 5043125584 -->\n",
       "<g class=\"node\" id=\"node7\">\n",
       "<title>5043125584</title>\n",
       "<polygon fill=\"none\" points=\"164.6621,-81.5 164.6621,-125.5 429.2246,-125.5 429.2246,-81.5 164.6621,-81.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"231.9468\" y=\"-99.3\">reshape_22: Reshape</text>\n",
       "<polyline fill=\"none\" points=\"299.2314,-81.5 299.2314,-125.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"327.0659\" y=\"-110.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"299.2314,-103.5 354.9004,-103.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"327.0659\" y=\"-88.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"354.9004,-81.5 354.9004,-125.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"392.0625\" y=\"-110.3\">(300, 1, 1)</text>\n",
       "<polyline fill=\"none\" points=\"354.9004,-103.5 429.2246,-103.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"392.0625\" y=\"-88.3\">(300, 1)</text>\n",
       "</g>\n",
       "<!-- 5043124368&#45;&gt;5043125584 -->\n",
       "<g class=\"edge\" id=\"edge7\">\n",
       "<title>5043124368-&gt;5043125584</title>\n",
       "<path d=\"M296.9434,-162.3664C296.9434,-154.1516 296.9434,-144.6579 296.9434,-135.7252\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"300.4435,-135.6068 296.9434,-125.6068 293.4435,-135.6069 300.4435,-135.6068\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 5043125264 -->\n",
       "<g class=\"node\" id=\"node8\">\n",
       "<title>5043125264</title>\n",
       "<polygon fill=\"none\" points=\"186.8208,-.5 186.8208,-44.5 407.0659,-44.5 407.0659,-.5 186.8208,-.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"238.9468\" y=\"-18.3\">dense_8: Dense</text>\n",
       "<polyline fill=\"none\" points=\"291.0728,-.5 291.0728,-44.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"318.9072\" y=\"-29.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"291.0728,-22.5 346.7417,-22.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"318.9072\" y=\"-7.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"346.7417,-.5 346.7417,-44.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"376.9038\" y=\"-29.3\">(300, 1)</text>\n",
       "<polyline fill=\"none\" points=\"346.7417,-22.5 407.0659,-22.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"376.9038\" y=\"-7.3\">(300, 1)</text>\n",
       "</g>\n",
       "<!-- 5043125584&#45;&gt;5043125264 -->\n",
       "<g class=\"edge\" id=\"edge8\">\n",
       "<title>5043125584-&gt;5043125264</title>\n",
       "<path d=\"M296.9434,-81.3664C296.9434,-73.1516 296.9434,-63.6579 296.9434,-54.7252\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"300.4435,-54.6068 296.9434,-44.6068 293.4435,-54.6069 300.4435,-54.6068\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A nicer visualization of the word2vec skipgram model used here.\n",
    "plot_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get to the validation we want to track, we'll initialize a secondary model. This model should share the the embedding layer with the primary model.  We aren't actually going to\n",
    "# train this model, so we don't need to compile it.\n",
    "validation_model = keras.Model(inputs=[input_target, input_context], outputs=similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the similarity callback objecct\n",
    "sim_cb = SimilarityCallback(valid_size=valid_size, \n",
    "                            dictionary=dictionary, \n",
    "                            valid_examples=valid_examples, \n",
    "                            vocab_size=vocab_size, \n",
    "                            validation_model=validation_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, loss = 0.69817429781\n",
      "Nearest to aboard:  font, way, being, vector, reviewed, hoped, notably, demon,\n",
      "Nearest to absorption:  editions, opinions, serves, defined, reportedly, unchanged, create, jonah,\n",
      "Nearest to abbot:  caliph, surnames, becomes, decorated, mate, encryption, thousand, even,\n",
      "Nearest to accidental:  colspan, filmed, counted, projected, paid, romans, momentum, bandwidth,\n",
      "Nearest to acclaimed:  hughes, lead, jain, august, observances, mouth, aka, recommendations,\n",
      "Nearest to abraham:  monastery, philosophers, albert, package, libertarian, gave, turn, defines,\n",
      "Nearest to ability:  bassist, several, mathematical, completed, dublin, causes, saudi, goat,\n",
      "Nearest to acid:  flesh, speculated, indonesian, fugue, false, seldom, range, hanover,\n",
      "Nearest to accepted:  malaria, ammunition, northeast, encountered, basketball, disorder, asteroids, chemical,\n",
      "Nearest to accused:  abu, empires, missouri, nights, county, love, neil, farms,\n",
      "Nearest to achilles:  location, kingdoms, viii, complicated, sales, factor, switch, onwards,\n",
      "Nearest to academic:  bound, emphasizes, acquisition, archer, feature, lamp, instruments, kb,\n",
      "Nearest to accidentally:  tat, texture, ted, resist, sierra, unusual, receives, capture,\n",
      "Nearest to aaron:  masters, quick, accept, classification, files, shortwave, cartoonist, forming,\n",
      "Nearest to accords:  hurt, traditional, recovery, cyril, preferences, contributing, arnold, milwaukee,\n",
      "Nearest to abdul:  manages, rent, boss, lectures, assembled, portrayed, norman, helicopter,\n",
      "Iteration 1000, loss = 0.664830029011\n",
      "Iteration 2000, loss = 0.695128142834\n",
      "Iteration 3000, loss = 0.700244545937\n",
      "Iteration 4000, loss = 0.678515195847\n",
      "Iteration 5000, loss = 0.668430149555\n",
      "Iteration 6000, loss = 0.690354406834\n",
      "Iteration 7000, loss = 0.692553818226\n",
      "Iteration 8000, loss = 0.692761838436\n",
      "Iteration 9000, loss = 0.738080024719\n",
      "Iteration 10000, loss = 0.633899390697\n",
      "Nearest to aboard:  font, cambridge, separation, viable, vector, babylonian, being, notably,\n",
      "Nearest to absorption:  spots, editions, defined, unchanged, travels, jonah, israelites, leon,\n",
      "Nearest to abbot:  becomes, caliph, encryption, bonaparte, constituencies, fruit, directions, excel,\n",
      "Nearest to accidental:  filmed, colspan, mann, displaced, plan, interests, regions, dutch,\n",
      "Nearest to acclaimed:  hughes, mount, mouth, demonstrated, entrepreneur, observances, aka, lead,\n",
      "Nearest to abraham:  monastery, libertarian, symbols, defines, philosophers, instructions, albert, pictures,\n",
      "Nearest to ability:  bassist, romania, integers, mathematical, goat, causes, several, edges,\n",
      "Nearest to acid:  joseph, speculated, flesh, maximum, butler, indonesian, false, trans,\n",
      "Nearest to accepted:  lyrics, attract, northeast, scotland, malaria, fossil, enough, display,\n",
      "Nearest to accused:  abu, fusion, empires, missouri, county, nights, brigades, sacred,\n",
      "Nearest to achilles:  kingdoms, location, ben, controls, rational, approximately, shaped, solo,\n",
      "Nearest to academic:  bound, acquisition, archer, realized, feature, instruments, kb, mean,\n",
      "Nearest to accidentally:  sierra, tat, unusual, began, capture, lenin, ted, swept,\n",
      "Nearest to aaron:  numerical, shortwave, ethnicity, bind, shadow, align, files, stop,\n",
      "Nearest to accords:  preferences, ber, youth, recovery, isn, classified, acquisition, samuel,\n",
      "Nearest to abdul:  portrayed, lectures, assembled, manages, slope, rent, boss, helicopter,\n",
      "Iteration 11000, loss = 0.716857016087\n",
      "Iteration 12000, loss = 0.739962756634\n",
      "Iteration 13000, loss = 0.712436318398\n",
      "Iteration 14000, loss = 0.708068907261\n",
      "Iteration 15000, loss = 0.691257834435\n",
      "Iteration 16000, loss = 0.707215845585\n",
      "Iteration 17000, loss = 0.640026271343\n",
      "Iteration 18000, loss = 0.686478972435\n",
      "Iteration 19000, loss = 0.754377603531\n",
      "Iteration 20000, loss = 0.661401331425\n",
      "Nearest to aboard:  stability, skill, house, babylonian, font, notably, instead, important,\n",
      "Nearest to absorption:  defined, spots, serves, leon, jonah, unchanged, editions, dish,\n",
      "Nearest to abbot:  caliph, encryption, music, bonaparte, surnames, incorporate, constituencies, fruit,\n",
      "Nearest to accidental:  mann, filmed, elliptic, lex, colspan, displaced, markup, helix,\n",
      "Nearest to acclaimed:  compound, lead, hughes, demonstrated, provides, performances, aka, lindy,\n",
      "Nearest to abraham:  monastery, instructions, libertarian, tomorrow, keeps, philosophers, package, symbols,\n",
      "Nearest to ability:  bassist, several, mathematical, joined, saudi, transmitted, edges, named,\n",
      "Nearest to acid:  reviewed, butler, audiences, manner, flesh, indonesian, trans, hit,\n",
      "Nearest to accepted:  months, malaria, fossil, icons, shirt, basketball, display, timber,\n",
      "Nearest to accused:  fusion, abu, neil, philippines, occupy, traces, usd, interfere,\n",
      "Nearest to achilles:  vegetables, chemical, location, kingdoms, anger, arise, implied, stayed,\n",
      "Nearest to academic:  acquisition, kb, bound, connecting, archer, aesthetics, constitutions, instruments,\n",
      "Nearest to accidentally:  tat, corresponds, ted, controversial, sierra, unusual, capture, liturgy,\n",
      "Nearest to aaron:  forming, rule, dominant, lie, stopped, bind, commander, out,\n",
      "Nearest to accords:  fires, preferences, samuel, ber, traditional, eugene, quantity, classified,\n",
      "Nearest to abdul:  lectures, rent, helicopter, connecting, buchanan, assembled, manages, reserved,\n",
      "Iteration 21000, loss = 0.74115228653\n",
      "Iteration 22000, loss = 0.703442752361\n",
      "Iteration 23000, loss = 0.703747570515\n",
      "Iteration 24000, loss = 0.409720093012\n",
      "Iteration 25000, loss = 0.623306155205\n",
      "Iteration 26000, loss = 0.241076588631\n",
      "Iteration 27000, loss = 0.624315023422\n",
      "Iteration 28000, loss = 0.544445574284\n",
      "Iteration 29000, loss = 0.591178417206\n",
      "Iteration 30000, loss = 0.784196257591\n",
      "Nearest to aboard:  stability, separation, reforms, font, skill, naturally, glacier, house,\n",
      "Nearest to absorption:  travels, spots, usual, lithium, auto, emphasis, defined, incidents,\n",
      "Nearest to abbot:  encryption, caliph, music, incorporate, fruit, constituencies, excel, broadcasting,\n",
      "Nearest to accidental:  amos, colspan, lex, riots, rhythm, constellation, normandy, buchanan,\n",
      "Nearest to acclaimed:  lead, hughes, entrepreneur, tyler, measurement, simply, aka, lindy,\n",
      "Nearest to abraham:  monastery, albert, flower, keeps, series, orbits, libertarian, philosophers,\n",
      "Nearest to ability:  london, romania, several, renewed, defensive, pump, baltic, joined,\n",
      "Nearest to acid:  audiences, accepted, euler, secular, forbidden, surname, tertiary, gift,\n",
      "Nearest to accepted:  shirt, months, acid, basketball, tuberculosis, office, display, malaria,\n",
      "Nearest to accused:  abu, fusion, neil, jeff, occupy, usd, interfere, oneself,\n",
      "Nearest to achilles:  virtue, vegetables, bus, lawyers, colony, chemical, stayed, markets,\n",
      "Nearest to academic:  acquisition, bound, aesthetics, connecting, kb, archer, instruments, constitutions,\n",
      "Nearest to accidentally:  liturgy, swept, tide, know, advised, drainage, associated, buckingham,\n",
      "Nearest to aaron:  forming, rule, several, diagnosed, presently, ethnicity, stopped, stop,\n",
      "Nearest to accords:  ber, virtual, vehicle, ignorance, fires, define, eugene, retaining,\n",
      "Nearest to abdul:  lectures, connecting, helicopter, email, rent, fe, buchanan, reserved,\n",
      "Iteration 31000, loss = 0.776800870895\n",
      "Iteration 32000, loss = 0.587319672108\n",
      "Iteration 33000, loss = 0.683923840523\n",
      "Iteration 34000, loss = 0.173846274614\n",
      "Iteration 35000, loss = 0.627019882202\n",
      "Iteration 36000, loss = 0.801794409752\n",
      "Iteration 37000, loss = 0.68433123827\n",
      "Iteration 38000, loss = 0.552802324295\n",
      "Iteration 39000, loss = 0.719568431377\n",
      "Iteration 40000, loss = 0.609778523445\n",
      "Nearest to aboard:  stability, skill, font, house, ones, naturally, reforms, babylonian,\n",
      "Nearest to absorption:  rio, reportedly, defined, strategy, easily, israelites, lithium, personnel,\n",
      "Nearest to abbot:  caliph, encryption, music, constituencies, princess, bonaparte, excel, fruit,\n",
      "Nearest to accidental:  rhythm, levy, riots, shore, altogether, colspan, viewing, normandy,\n",
      "Nearest to acclaimed:  lead, simply, vessel, transition, compound, entrepreneur, essentially, aka,\n",
      "Nearest to abraham:  monastery, philosophers, tomorrow, ddr, np, provisions, keeps, mathematicians,\n",
      "Nearest to ability:  appears, full, purpose, ships, completed, loses, motorola, past,\n",
      "Nearest to acid:  trans, raf, memphis, maximum, gift, prediction, salt, indonesian,\n",
      "Nearest to accepted:  added, encountered, regime, malaria, shirt, scotland, basketball, enough,\n",
      "Nearest to accused:  neil, surrounding, filter, oneself, occupy, cats, abu, fusion,\n",
      "Nearest to achilles:  vegetables, news, virtue, sales, bus, stayed, banks, lawyers,\n",
      "Nearest to academic:  bound, aesthetics, leon, acquisition, kb, connecting, cord, affects,\n",
      "Nearest to accidentally:  know, liturgy, drainage, tide, resist, unusual, trained, lose,\n",
      "Nearest to aaron:  forming, ethnicity, material, hotels, credited, rule, several, presently,\n",
      "Nearest to accords:  vehicle, cemetery, traditional, lambda, promise, st, eugene, stream,\n",
      "Nearest to abdul:  lectures, weekend, buchanan, sees, frontier, bolshevik, interpretation, lincoln,\n",
      "Iteration 41000, loss = 0.758912742138\n",
      "Iteration 42000, loss = 0.772502422333\n",
      "Iteration 43000, loss = 0.403905361891\n",
      "Iteration 44000, loss = 0.449045866728\n",
      "Iteration 45000, loss = 0.280706465244\n",
      "Iteration 46000, loss = 0.212463662028\n",
      "Iteration 47000, loss = 0.211306869984\n",
      "Iteration 48000, loss = 0.511755347252\n",
      "Iteration 49000, loss = 0.575158596039\n"
     ]
    }
   ],
   "source": [
    "# Now we can train this thing.\n",
    "arr_1 = np.zeros((1,))\n",
    "arr_2 = np.zeros((1,))\n",
    "arr_3 = np.zeros((1,))\n",
    "for cnt in range(epochs):\n",
    "    idx = np.random.randint(0, len(labels)-1)\n",
    "    arr_1[0,] = word_target[idx]\n",
    "    arr_2[0,] = word_context[idx]\n",
    "    arr_3[0,] = labels[idx]\n",
    "    loss = model.train_on_batch([arr_1, arr_2], arr_3)\n",
    "    if cnt % 1000 == 0:\n",
    "        print('Iteration {0}, loss = {1}'.format(cnt, loss))\n",
    "    if cnt % 10000 == 0:\n",
    "        sim_cb.run_sim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest to six:  stations, impulse, debut, ivoire, burst, bang, bodily, moore,\n",
      "Nearest to eight:  intimate, augustine, inserted, genealogy, wax, winds, kennedy, favoured,\n"
     ]
    }
   ],
   "source": [
    "predict_on_validation_model(word_list=['six', 'eight'], dictionary=dictionary, reversed_dictionary=reversed_dictionary, validation_model=validation_model, vocab_size=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:basic_ml]",
   "language": "python",
   "name": "conda-env-basic_ml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
