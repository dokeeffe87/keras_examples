{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec in keras. \n",
    "* based partially on: https://adventuresinmachinelearning.com/word2vec-keras-tutorial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll implement the skip-gram model first (CBOW later)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that to implement negative sampling, we'll follow the following procedure:\n",
    "* Construct a binary output layer instead of multi-class\n",
    "* Output is True if we are suppling the target word and a true context word\n",
    "* Output is False if we are suppling the target word and a a negative sample (i.e. a word sampled which is NOT in the context of the target)\n",
    "* To ensure that similar words end up having similar embeddings, we'll use cosine similarity. This should give a 1 when two words are in the same context and 0 otherwise\n",
    "Broadly, the network will look like:\n",
    "* An integer input (i.e. the index of the target word in the vocabulary) and a true or false context word\n",
    "* An embedding layer lookup (i.e. looking up the embedding vector in the embedding matrix)\n",
    "* The dot product operation (i.e. to compute the cosine similarity)\n",
    "* The output layer: a sigmoid here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules:\n",
    "from __future__ import division\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import keras\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "import functools\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from IPython.display import SVG\n",
    "from keras_sequential_ascii import sequential_model_to_ascii_printout\n",
    "from keras.callbacks import History \n",
    "import urllib\n",
    "import zipfile\n",
    "import tensorflow as tf\n",
    "import collections\n",
    "sys.path.insert(0, '/Users/danielokeeffe/Documents/src/nlp_preprocessing/')\n",
    "import nlp_preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'nlp_preprocessing' from '/Users/danielokeeffe/Documents/src/nlp_preprocessing/nlp_preprocessing.py'>"
      ]
     },
     "execution_count": 572,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reload nlp_preprocessing if you add any functions to it\n",
    "reload(nlp_preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimilarityCallback:\n",
    "    def __init__(self, valid_size, dictionary, valid_examples, vocab_size, validation_model):\n",
    "        self.valid_size = valid_size\n",
    "        self.dictionary = dictionary\n",
    "        self.valid_examples = valid_examples\n",
    "        self.vocab_size = vocab_size\n",
    "        self.validation_model = validation_model\n",
    "        \n",
    "    def run_sim(self):\n",
    "        for i in range(self.valid_size):\n",
    "            valid_word = self.dictionary[self.valid_examples[i]]\n",
    "            # Define the number of nearest neighbours\n",
    "            top_k = 8\n",
    "            sim = self._get_sim(self.valid_examples[i])\n",
    "            nearest = (-sim).argsort()[1:top_k+1]\n",
    "            log_str = 'Nearest to {0}: '.format(valid_word)\n",
    "            for k in range(top_k):\n",
    "                close_word = self.dictionary[nearest[k]]\n",
    "                log_str = '{0} {1},'.format(log_str, close_word)\n",
    "            print(log_str)\n",
    "            \n",
    "\n",
    "    def _get_sim(self, valid_word_idx):\n",
    "        sim = np.zeros((self.vocab_size,))\n",
    "        in_arr1 = np.zeros((1,))\n",
    "        in_arr2 = np.zeros((1,))\n",
    "        for i in range(self.vocab_size):\n",
    "            in_arr1[0,] = valid_word_idx\n",
    "            in_arr2[0,] = i\n",
    "            out = self.validation_model.predict_on_batch([in_arr1, in_arr2])\n",
    "            sim[i] = out\n",
    "        return sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model(model):\n",
    "    return SVG(keras.utils.vis_utils.model_to_dot(model, show_shapes=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_on_validation_model(word_list, dictionary, reversed_dictionary, validation_model, vocab_size, top_k=8):\n",
    "    \"\"\"A function to return the top_k most similar words after training of the primary model\"\"\"\n",
    "    # Verify that the words in the input word list are actually in the dictionary\n",
    "    try:\n",
    "        print(word_list)\n",
    "        word_indices = [reversed_dictionary[word] for word in word_list]\n",
    "        print(word_indices)\n",
    "    except KeyError:\n",
    "        print('a supplied word is not in dictionary')\n",
    "        return None\n",
    "    for i in range(len(word_indices)):\n",
    "        test_word = word_list[i]\n",
    "        sim = validation_get_sim(valid_word_idx=word_indices[i], \n",
    "                                 validation_model=validation_model, \n",
    "                                 vocab_size=vocab_size)\n",
    "        nearest = (-sim).argsort()[1:top_k+1]\n",
    "        log_str = 'Nearest to {0}: '.format(test_word)\n",
    "        for k in range(top_k):\n",
    "            close_word = dictionary[nearest[k]]\n",
    "            log_str = '{0} {1},'.format(log_str, close_word)\n",
    "        print(log_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_get_sim(valid_word_idx, validation_model, vocab_size):\n",
    "    sim = np.zeros((vocab_size,))\n",
    "    in_arr1 = np.zeros((1,))\n",
    "    in_arr2 = np.zeros((1,))\n",
    "    for i in range(vocab_size):\n",
    "        in_arr1[0,] = valid_word_idx\n",
    "        in_arr2[0,] = i\n",
    "        # out = validation_model.predict_on_batch([in_arr1, in_arr2])\n",
    "        out = validation_model.predict([in_arr1, in_arr2])\n",
    "        sim[i] = out\n",
    "    return sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data.  We'll use the text data at: 'http://mattmahoney.net/dc/' The filename is text8.zip\n",
    "url = 'http://mattmahoney.net/dc/'\n",
    "filename = download_file('text8.zip', url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = nlp_preprocessing.download_file('text8.zip', url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = nlp_preprocessing.read_data_from_zip(filename=filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 520,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17005207"
      ]
     },
     "execution_count": 524,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocabulary[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is pretty big, so we'll produce a trimmed version of the vocabulary of the n most common words, say 10,000\n",
    "vocab_size = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary, reversed_dictionary = nlp_preprocessing.build_data_for_keras_skipgram(vocabulary=vocabulary[0], vocabulary_size=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 574,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 575,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reversed_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 576,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reversed_dictionary['a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a'"
      ]
     },
     "execution_count": 577,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fall'"
      ]
     },
     "execution_count": 578,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary[1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 579,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reversed_dictionary['fall']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the'"
      ]
     },
     "execution_count": 580,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'panda'"
      ]
     },
     "execution_count": 581,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary[9999]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 582,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reversed_dictionary['UNK']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before we delete the vocabulary, we need the sequence of words (i.e. vocabulary) translated into indices.  This is necessary for the skipgram selection below.\n",
    "sequence_of_indices, unk_count = nlp_preprocessing.make_sequence_of_indices_from_vocabulary(vocabulary=vocabulary, reversed_dictionary=reversed_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 584,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sequence_of_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17005207"
      ]
     },
     "execution_count": 585,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sequence_of_indices[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 586,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unk_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1737307]"
      ]
     },
     "execution_count": 587,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unk_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10216323741310529"
      ]
     },
     "execution_count": 588,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Which is \n",
    "1737307/17005207"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5239, 3084, 12, 6, 195, 2, 3137, 46, 59, 156]"
      ]
     },
     "execution_count": 589,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# About 10.2% unk...\n",
    "sequence_of_indices[0][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['anarchism',\n",
       " 'originated',\n",
       " 'as',\n",
       " 'a',\n",
       " 'term',\n",
       " 'of',\n",
       " 'abuse',\n",
       " 'first',\n",
       " 'used',\n",
       " 'against']"
      ]
     },
     "execution_count": 590,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary[0][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anarchism\n",
      "originated\n",
      "as\n",
      "a\n",
      "term\n",
      "of\n",
      "abuse\n",
      "first\n",
      "used\n",
      "against\n"
     ]
    }
   ],
   "source": [
    "for idx in sequence_of_indices[0][0:10]:\n",
    "    print(dictionary[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ok great, so that should preserve the sequence information we'll need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So now we don't need the full vocabulary anymore.\n",
    "del vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define some basic parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the window of words around the target to will be used to draw context samples:\n",
    "window_size = 3\n",
    "# The size of each word embedding vector:\n",
    "vector_dim = 300\n",
    "# The number of epochs to train for. This is going to be big.  Even with negative sampling, creating a decent embedding matrix can take awhile\n",
    "epochs = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll need to setup a validation set. This will be used to check to see what words grow in similarity as we train the network.\n",
    "# We'll select a random set of words to measure similarity on\n",
    "valid_size = 16\n",
    "# We want to select these words from the top 100 most common words in the dataset.  We didn't explicitly create the dictionary this way...we'll do it at random for now...\n",
    "valid_window = 100\n",
    "valid_examples = np.random.choice(valid_window, valid_size, replace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement skipgram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The sampling table will make sure we sample both true and false context words in a balanced way and not just select the most common words\n",
    "sampling_table = keras.preprocessing.sequence.make_sampling_table(vocab_size)\n",
    "# We need to supply the sequence of indices or the text.  It's only one document, so we'll send just the zeroth element of sequence_of_indices \n",
    "couples, labels = keras.preprocessing.sequence.skipgrams(sequence_of_indices[0], vocab_size, window_size=window_size, sampling_table=sampling_table)\n",
    "word_target, word_context = zip(*couples)\n",
    "word_target = np.array(word_target, dtype='int32')\n",
    "word_context = np.array(word_context, dtype='int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([[1303, 26], [2740, 31], [6266, 299], [8247, 7053], [2080, 3834], [4996, 4366], [641, 682], [507, 6498], [1307, 2732], [8302, 3009]], [1, 1, 0, 0, 0, 0, 1, 0, 1, 0])\n"
     ]
    }
   ],
   "source": [
    "print(couples[:10], labels[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To actually implement the embedding layers, we'll need to use the functional API.  We need to share a single embedding layer between two tensors (target word and context \n",
    "# words), and an additional output to measure similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create input variables, one for the target, one for the context. We're just going to supply the target word and a context word individually, so the size of the inputs are both 1.\n",
    "input_target = keras.Input((1,))\n",
    "input_context = keras.Input((1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the embedding layer (we should try out different initializers for the weights also):\n",
    "embedding = keras.layers.Embedding(vocab_size, vector_dim, input_length=1, name='embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we need to be able to lookup target and context words in the embedding layer to measure similarity.\n",
    "target = embedding(input_target)\n",
    "target = keras.layers.Reshape((vector_dim, 1))(target)\n",
    "context = embedding(input_context)\n",
    "context = keras.layers.Reshape((vector_dim, 1))(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the similarity measurement\n",
    "similarity = keras.layers.dot([target, context], normalize=True, axes=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_product = keras.layers.dot([target, context], normalize=False, axes=0)\n",
    "dot_product = keras.layers.Reshape((1,))(dot_product)\n",
    "output = keras.layers.Dense(1, activation='sigmoid')(dot_product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model = keras.Model(inputs=[input_target, input_context], outputs=output)\n",
    "model.compile(loss='binary_crossentropy', optimizer='rmsprop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_11 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_12 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 1, 300)       3000000     input_11[0][0]                   \n",
      "                                                                 input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "reshape_26 (Reshape)            (None, 300, 1)       0           embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_27 (Reshape)            (None, 300, 1)       0           embedding[1][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dot_22 (Dot)                    (300, 1, 1)          0           reshape_26[0][0]                 \n",
      "                                                                 reshape_27[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "reshape_28 (Reshape)            (300, 1)             0           dot_22[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (300, 1)             2           reshape_28[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 3,000,002\n",
      "Trainable params: 3,000,002\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           OPERATION           DATA DIMENSIONS   WEIGHTS(N)   WEIGHTS(%)\n",
      "\n",
      "               Input   #####           1\n",
      "          InputLayer     |   -------------------         0     0.0%\n",
      "                       #####           1\n",
      "          InputLayer     |   -------------------         0     0.0%\n",
      "                       #####           1\n",
      "           Embedding   emb | -------------------   3000000    99.0%\n",
      "                       #####      1  300\n",
      "             Reshape     |   -------------------         0     0.0%\n",
      "                       #####    300    1\n",
      "             Reshape     |   -------------------         0     0.0%\n",
      "                       #####    300    1\n",
      "                 Dot   ????? -------------------         0     0.0%\n",
      "                       #####      1    1\n",
      "             Reshape     |   -------------------         0     0.0%\n",
      "                       #####           1\n",
      "               Dense   XXXXX -------------------         2     0.0%\n",
      "             sigmoid   #####           1\n"
     ]
    }
   ],
   "source": [
    "sequential_model_to_ascii_printout(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"458pt\" viewBox=\"0.00 0.00 601.89 458.00\" width=\"602pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 454)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"#ffffff\" points=\"-4,4 -4,-454 597.8867,-454 597.8867,4 -4,4\" stroke=\"transparent\"/>\n",
       "<!-- 5068720528 -->\n",
       "<g class=\"node\" id=\"node1\">\n",
       "<title>5068720528</title>\n",
       "<polygon fill=\"none\" points=\"27.8599,-405.5 27.8599,-449.5 288.0269,-449.5 288.0269,-405.5 27.8599,-405.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"95.2847\" y=\"-423.3\">input_11: InputLayer</text>\n",
       "<polyline fill=\"none\" points=\"162.7095,-405.5 162.7095,-449.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"190.5439\" y=\"-434.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"162.7095,-427.5 218.3784,-427.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"190.5439\" y=\"-412.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"218.3784,-405.5 218.3784,-449.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"253.2026\" y=\"-434.3\">(None, 1)</text>\n",
       "<polyline fill=\"none\" points=\"218.3784,-427.5 288.0269,-427.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"253.2026\" y=\"-412.3\">(None, 1)</text>\n",
       "</g>\n",
       "<!-- 5247693072 -->\n",
       "<g class=\"node\" id=\"node3\">\n",
       "<title>5247693072</title>\n",
       "<polygon fill=\"none\" points=\"145.2139,-324.5 145.2139,-368.5 448.6729,-368.5 448.6729,-324.5 145.2139,-324.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"220.2847\" y=\"-342.3\">embedding: Embedding</text>\n",
       "<polyline fill=\"none\" points=\"295.3555,-324.5 295.3555,-368.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"323.1899\" y=\"-353.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"295.3555,-346.5 351.0244,-346.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"323.1899\" y=\"-331.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"351.0244,-324.5 351.0244,-368.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"399.8486\" y=\"-353.3\">(None, 1)</text>\n",
       "<polyline fill=\"none\" points=\"351.0244,-346.5 448.6729,-346.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"399.8486\" y=\"-331.3\">(None, 1, 300)</text>\n",
       "</g>\n",
       "<!-- 5068720528&#45;&gt;5247693072 -->\n",
       "<g class=\"edge\" id=\"edge1\">\n",
       "<title>5068720528-&gt;5247693072</title>\n",
       "<path d=\"M195.9257,-405.3664C212.6275,-395.6337 232.4102,-384.1057 250.0225,-373.8424\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"252.1291,-376.6658 259.0069,-368.6068 248.6047,-370.6177 252.1291,-376.6658\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 5068719760 -->\n",
       "<g class=\"node\" id=\"node2\">\n",
       "<title>5068719760</title>\n",
       "<polygon fill=\"none\" points=\"305.6035,-405.5 305.6035,-449.5 566.2832,-449.5 566.2832,-405.5 305.6035,-405.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"373.2847\" y=\"-423.3\">input_12: InputLayer</text>\n",
       "<polyline fill=\"none\" points=\"440.9658,-405.5 440.9658,-449.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"468.8003\" y=\"-434.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"440.9658,-427.5 496.6348,-427.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"468.8003\" y=\"-412.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"496.6348,-405.5 496.6348,-449.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"531.459\" y=\"-434.3\">(None, 1)</text>\n",
       "<polyline fill=\"none\" points=\"496.6348,-427.5 566.2832,-427.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"531.459\" y=\"-412.3\">(None, 1)</text>\n",
       "</g>\n",
       "<!-- 5068719760&#45;&gt;5247693072 -->\n",
       "<g class=\"edge\" id=\"edge2\">\n",
       "<title>5068719760-&gt;5247693072</title>\n",
       "<path d=\"M397.961,-405.3664C381.2592,-395.6337 361.4766,-384.1057 343.8643,-373.8424\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"345.2821,-370.6177 334.8798,-368.6068 341.7576,-376.6658 345.2821,-370.6177\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 5248839120 -->\n",
       "<g class=\"node\" id=\"node4\">\n",
       "<title>5248839120</title>\n",
       "<polygon fill=\"none\" points=\"0,-243.5 0,-287.5 287.8867,-287.5 287.8867,-243.5 0,-243.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"67.2847\" y=\"-261.3\">reshape_26: Reshape</text>\n",
       "<polyline fill=\"none\" points=\"134.5693,-243.5 134.5693,-287.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"162.4038\" y=\"-272.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"134.5693,-265.5 190.2383,-265.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"162.4038\" y=\"-250.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"190.2383,-243.5 190.2383,-287.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"239.0625\" y=\"-272.3\">(None, 1, 300)</text>\n",
       "<polyline fill=\"none\" points=\"190.2383,-265.5 287.8867,-265.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"239.0625\" y=\"-250.3\">(None, 300, 1)</text>\n",
       "</g>\n",
       "<!-- 5247693072&#45;&gt;5248839120 -->\n",
       "<g class=\"edge\" id=\"edge3\">\n",
       "<title>5247693072-&gt;5248839120</title>\n",
       "<path d=\"M255.1354,-324.3664C236.4985,-314.4998 214.3764,-302.7881 194.7911,-292.4194\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"196.1762,-289.1925 185.7007,-287.6068 192.901,-295.379 196.1762,-289.1925\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 5003976208 -->\n",
       "<g class=\"node\" id=\"node5\">\n",
       "<title>5003976208</title>\n",
       "<polygon fill=\"none\" points=\"306,-243.5 306,-287.5 593.8867,-287.5 593.8867,-243.5 306,-243.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"373.2847\" y=\"-261.3\">reshape_27: Reshape</text>\n",
       "<polyline fill=\"none\" points=\"440.5693,-243.5 440.5693,-287.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"468.4038\" y=\"-272.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"440.5693,-265.5 496.2383,-265.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"468.4038\" y=\"-250.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"496.2383,-243.5 496.2383,-287.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"545.0625\" y=\"-272.3\">(None, 1, 300)</text>\n",
       "<polyline fill=\"none\" points=\"496.2383,-265.5 593.8867,-265.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"545.0625\" y=\"-250.3\">(None, 300, 1)</text>\n",
       "</g>\n",
       "<!-- 5247693072&#45;&gt;5003976208 -->\n",
       "<g class=\"edge\" id=\"edge4\">\n",
       "<title>5247693072-&gt;5003976208</title>\n",
       "<path d=\"M338.7513,-324.3664C357.3882,-314.4998 379.5104,-302.7881 399.0956,-292.4194\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"400.9857,-295.379 408.186,-287.6068 397.7105,-289.1925 400.9857,-295.379\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 5076076944 -->\n",
       "<g class=\"node\" id=\"node6\">\n",
       "<title>5076076944</title>\n",
       "<polygon fill=\"none\" points=\"129.6587,-162.5 129.6587,-206.5 464.228,-206.5 464.228,-162.5 129.6587,-162.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"171.2983\" y=\"-180.3\">dot_22: Dot</text>\n",
       "<polyline fill=\"none\" points=\"212.938,-162.5 212.938,-206.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"240.7725\" y=\"-191.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"212.938,-184.5 268.6069,-184.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"240.7725\" y=\"-169.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"268.6069,-162.5 268.6069,-206.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"366.4175\" y=\"-191.3\">[(None, 300, 1), (None, 300, 1)]</text>\n",
       "<polyline fill=\"none\" points=\"268.6069,-184.5 464.228,-184.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"366.269\" y=\"-169.3\">(300, 1, 1)</text>\n",
       "</g>\n",
       "<!-- 5248839120&#45;&gt;5076076944 -->\n",
       "<g class=\"edge\" id=\"edge5\">\n",
       "<title>5248839120-&gt;5076076944</title>\n",
       "<path d=\"M185.7513,-243.3664C204.3882,-233.4998 226.5104,-221.7881 246.0956,-211.4194\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"247.9857,-214.379 255.186,-206.6068 244.7105,-208.1925 247.9857,-214.379\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 5003976208&#45;&gt;5076076944 -->\n",
       "<g class=\"edge\" id=\"edge6\">\n",
       "<title>5003976208-&gt;5076076944</title>\n",
       "<path d=\"M408.1354,-243.3664C389.4985,-233.4998 367.3764,-221.7881 347.7911,-211.4194\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"349.1762,-208.1925 338.7007,-206.6068 345.901,-214.379 349.1762,-208.1925\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 5076076880 -->\n",
       "<g class=\"node\" id=\"node7\">\n",
       "<title>5076076880</title>\n",
       "<polygon fill=\"none\" points=\"164.6621,-81.5 164.6621,-125.5 429.2246,-125.5 429.2246,-81.5 164.6621,-81.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"231.9468\" y=\"-99.3\">reshape_28: Reshape</text>\n",
       "<polyline fill=\"none\" points=\"299.2314,-81.5 299.2314,-125.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"327.0659\" y=\"-110.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"299.2314,-103.5 354.9004,-103.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"327.0659\" y=\"-88.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"354.9004,-81.5 354.9004,-125.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"392.0625\" y=\"-110.3\">(300, 1, 1)</text>\n",
       "<polyline fill=\"none\" points=\"354.9004,-103.5 429.2246,-103.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"392.0625\" y=\"-88.3\">(300, 1)</text>\n",
       "</g>\n",
       "<!-- 5076076944&#45;&gt;5076076880 -->\n",
       "<g class=\"edge\" id=\"edge7\">\n",
       "<title>5076076944-&gt;5076076880</title>\n",
       "<path d=\"M296.9434,-162.3664C296.9434,-154.1516 296.9434,-144.6579 296.9434,-135.7252\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"300.4435,-135.6068 296.9434,-125.6068 293.4435,-135.6069 300.4435,-135.6068\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 5076077008 -->\n",
       "<g class=\"node\" id=\"node8\">\n",
       "<title>5076077008</title>\n",
       "<polygon fill=\"none\" points=\"183.3208,-.5 183.3208,-44.5 410.5659,-44.5 410.5659,-.5 183.3208,-.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"238.9468\" y=\"-18.3\">dense_10: Dense</text>\n",
       "<polyline fill=\"none\" points=\"294.5728,-.5 294.5728,-44.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"322.4072\" y=\"-29.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"294.5728,-22.5 350.2417,-22.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"322.4072\" y=\"-7.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"350.2417,-.5 350.2417,-44.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"380.4038\" y=\"-29.3\">(300, 1)</text>\n",
       "<polyline fill=\"none\" points=\"350.2417,-22.5 410.5659,-22.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"380.4038\" y=\"-7.3\">(300, 1)</text>\n",
       "</g>\n",
       "<!-- 5076076880&#45;&gt;5076077008 -->\n",
       "<g class=\"edge\" id=\"edge8\">\n",
       "<title>5076076880-&gt;5076077008</title>\n",
       "<path d=\"M296.9434,-81.3664C296.9434,-73.1516 296.9434,-63.6579 296.9434,-54.7252\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"300.4435,-54.6068 296.9434,-44.6068 293.4435,-54.6069 300.4435,-54.6068\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 606,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A nicer visualization of the word2vec skipgram model used here.\n",
    "plot_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get to the validation we want to track, we'll initialize a secondary model. This model should share the the embedding layer with the primary model.  We aren't actually going to\n",
    "# train this model, so we don't need to compile it.\n",
    "validation_model = keras.Model(inputs=[input_target, input_context], outputs=similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the similarity callback objecct\n",
    "sim_cb = SimilarityCallback(valid_size=valid_size, \n",
    "                            dictionary=dictionary, \n",
    "                            valid_examples=valid_examples, \n",
    "                            vocab_size=vocab_size, \n",
    "                            validation_model=validation_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, loss = 0.697695851326\n",
      "Nearest to from:  delaware, favourite, luxembourg, db, calculate, shift, value, retain,\n",
      "Nearest to no:  null, stuart, map, webster, traditional, blend, universally, know,\n",
      "Nearest to so:  slavery, potentially, locomotives, tensions, louis, handful, contracted, study,\n",
      "Nearest to of:  papers, fortran, shoes, physicists, prepared, names, balance, greek,\n",
      "Nearest to however:  motor, absorbed, openly, gov, coach, algebraic, tcp, studies,\n",
      "Nearest to at:  woman, rolls, prosecution, masculine, aged, amino, heroin, series,\n",
      "Nearest to had:  officials, israelites, week, carlos, grande, possibility, variant, substance,\n",
      "Nearest to use:  solids, dam, finally, comment, proud, correctly, delhi, hp,\n",
      "Nearest to i:  successive, rangle, compatible, cameras, boeing, burke, sand, vessel,\n",
      "Nearest to are:  centralized, herzegovina, upcoming, elevator, smoking, couples, recall, swiss,\n",
      "Nearest to first:  tense, testament, ultra, denominations, calculation, emancipation, species, zoo,\n",
      "Nearest to used:  my, hugo, reflected, activist, cricket, personality, goods, curtis,\n",
      "Nearest to to:  helps, jumps, terrorists, beverages, sunk, like, struggles, algebra,\n",
      "Nearest to often:  hegel, steadily, aikido, encounter, hermes, commons, helena, imagine,\n",
      "Nearest to that:  contributors, reflection, trigger, hellenic, opponent, components, trick, resolution,\n",
      "Nearest to two:  believers, provisional, tanker, providers, scope, aerospace, trans, aloe,\n",
      "Iteration 1000, loss = 0.724118947983\n",
      "Iteration 2000, loss = 0.721069395542\n",
      "Iteration 3000, loss = 0.681323945522\n",
      "Iteration 4000, loss = 0.694306373596\n",
      "Iteration 5000, loss = 0.698995649815\n",
      "Iteration 6000, loss = 0.686193108559\n",
      "Iteration 7000, loss = 0.682769656181\n",
      "Iteration 8000, loss = 0.679930329323\n",
      "Iteration 9000, loss = 0.725741505623\n",
      "Iteration 10000, loss = 0.686805546284\n",
      "Nearest to from:  retain, eventually, susceptible, hull, studying, merchant, ludwig, ordered,\n",
      "Nearest to no:  universally, rings, ecology, challenged, rows, violations, abbreviations, speeches,\n",
      "Nearest to so:  everyone, slavery, potentially, debated, constitution, emerged, mining, handful,\n",
      "Nearest to of:  zero, inability, places, asian, insubstantial, the, ally, between,\n",
      "Nearest to however:  gov, motor, homogeneous, below, fierce, assyrian, doubles, coach,\n",
      "Nearest to at:  stake, unless, profile, cos, is, study, moderate, covenant,\n",
      "Nearest to had:  situated, cent, braves, officials, extended, partners, contribution, chemical,\n",
      "Nearest to use:  solids, hp, condemned, later, troy, turns, hamilton, dada,\n",
      "Nearest to i:  perfect, devil, jerome, successive, releases, integral, trend, anthem,\n",
      "Nearest to are:  descriptions, title, burke, hospitals, canadian, statistics, featured, amsterdam,\n",
      "Nearest to first:  species, dot, hit, exceptional, recovery, centered, fantasy, blade,\n",
      "Nearest to used:  cologne, wavelength, reflected, j, al, fossils, sabbath, dominate,\n",
      "Nearest to to:  anonymous, uses, returned, cover, around, susan, funded, the,\n",
      "Nearest to often:  newer, hegel, encounter, toronto, geometry, separately, skull, uniform,\n",
      "Nearest to that:  oklahoma, structure, beast, proposition, ignored, pages, sentiment, conscription,\n",
      "Nearest to two:  games, three, cobain, afl, twelfth, nine, one, tanker,\n",
      "Iteration 11000, loss = 0.714972198009\n",
      "Iteration 12000, loss = 0.703311860561\n",
      "Iteration 13000, loss = 0.678294181824\n",
      "Iteration 14000, loss = 0.681680262089\n",
      "Iteration 15000, loss = 0.713292777538\n",
      "Iteration 16000, loss = 0.699892342091\n",
      "Iteration 17000, loss = 0.720375359058\n",
      "Iteration 18000, loss = 0.727491140366\n",
      "Iteration 19000, loss = 0.621127426624\n",
      "Iteration 20000, loss = 0.697566866875\n",
      "Nearest to from:  andes, course, the, contemporary, elf, could, separated, nineteenth,\n",
      "Nearest to no:  paved, ecology, longer, greece, webster, queens, rings, whatever,\n",
      "Nearest to so:  constitution, enabled, descent, matching, fiction, far, difficulties, diplomacy,\n",
      "Nearest to of:  nation, the, characteristic, asian, fusion, looking, organization, favour,\n",
      "Nearest to however:  bath, assyrian, minorities, permission, measuring, censorship, hale, desires,\n",
      "Nearest to at:  stake, god, cos, all, power, sponsored, ideals, medical,\n",
      "Nearest to had:  cent, illusion, situated, already, clark, theoretical, karate, restore,\n",
      "Nearest to use:  phrase, solids, duo, hamilton, hp, extends, clients, shelter,\n",
      "Nearest to i:  expressions, devil, perfect, burke, wireless, drum, jew, trend,\n",
      "Nearest to are:  statistics, burke, level, image, predecessors, faced, invited, surface,\n",
      "Nearest to first:  conference, dot, novels, port, dalek, rulers, immediate, mythology,\n",
      "Nearest to used:  denote, dominate, cologne, reflected, defend, additive, viral, han,\n",
      "Nearest to to:  returned, went, give, servant, economies, armies, what, produce,\n",
      "Nearest to often:  newer, hegel, diagrams, separately, commons, geometry, arises, pace,\n",
      "Nearest to that:  conscription, sentiment, code, yards, widow, penis, life, have,\n",
      "Nearest to two:  gear, three, seven, laid, guinness, twelfth, second, games,\n",
      "Iteration 21000, loss = 0.753626942635\n",
      "Iteration 22000, loss = 0.729927659035\n",
      "Iteration 23000, loss = 0.70606392622\n",
      "Iteration 24000, loss = 0.723016262054\n",
      "Iteration 25000, loss = 0.662096977234\n",
      "Iteration 26000, loss = 0.658821582794\n",
      "Iteration 27000, loss = 0.693808495998\n",
      "Iteration 28000, loss = 0.62234133482\n",
      "Iteration 29000, loss = 0.732186079025\n",
      "Iteration 30000, loss = 0.662516951561\n",
      "Nearest to from:  outward, the, andes, gamma, of, course, by, geological,\n",
      "Nearest to no:  ecology, abbreviations, clinton, challenged, rows, ivy, blend, restaurants,\n",
      "Nearest to so:  announces, promised, arrangement, bo, pr, identity, enabled, blocks,\n",
      "Nearest to of:  a, nation, the, and, nine, one, contemporaries, with,\n",
      "Nearest to however:  tertiary, sets, hale, hood, descent, christmas, finding, dos,\n",
      "Nearest to at:  god, stake, committed, unless, is, mozilla, blacks, rand,\n",
      "Nearest to had:  clinical, emblem, edwards, obtained, the, plan, journey, braves,\n",
      "Nearest to use:  solids, phrase, and, extends, hp, others, midi, chose,\n",
      "Nearest to i:  expressions, wrote, perfect, devil, drum, burke, millennium, painful,\n",
      "Nearest to are:  weight, spirits, beyond, sensitive, areas, burke, narrowly, predecessors,\n",
      "Nearest to first:  isomorphism, sons, mass, rulers, hit, chance, mythology, centres,\n",
      "Nearest to used:  denote, defend, additive, fill, continuum, o, wavelength, fossils,\n",
      "Nearest to to:  the, document, armies, subcontinent, a, only, biblical, returned,\n",
      "Nearest to often:  newer, justify, separately, hegel, broken, pace, fulfilled, wrestling,\n",
      "Nearest to that:  reactions, kent, code, conscription, hood, pointed, eusebius, spanish,\n",
      "Nearest to two:  gear, winning, mi, gregorian, seven, wimbledon, games, gdp,\n",
      "Iteration 31000, loss = 0.715696692467\n",
      "Iteration 32000, loss = 0.63434189558\n",
      "Iteration 33000, loss = 0.697481453419\n",
      "Iteration 34000, loss = 0.713123619556\n",
      "Iteration 35000, loss = 0.606387674809\n",
      "Iteration 36000, loss = 0.691680550575\n",
      "Iteration 37000, loss = 0.753059983253\n",
      "Iteration 38000, loss = 0.651744246483\n",
      "Iteration 39000, loss = 0.648197472095\n",
      "Iteration 40000, loss = 0.654274344444\n",
      "Nearest to from:  the, nine, by, seven, of, four, governmental, outward,\n",
      "Nearest to no:  washington, evidence, abbreviations, violations, stuart, universe, webster, started,\n",
      "Nearest to so:  that, announces, are, traditional, individually, not, correlation, pr,\n",
      "Nearest to of:  the, nation, is, and, a, nine, by, one,\n",
      "Nearest to however:  tertiary, public, tolerance, how, absorbed, worn, four, medical,\n",
      "Nearest to at:  stake, is, last, god, on, the, study, with,\n",
      "Nearest to had:  israelites, harbors, murray, monroe, been, genitive, po, formats,\n",
      "Nearest to use:  phrase, reserve, extends, s, drops, cameras, presented, the,\n",
      "Nearest to i:  expressions, devil, your, victim, wrote, painful, perfect, burke,\n",
      "Nearest to are:  drawing, weight, spirits, verbs, labels, tcp, narrowly, various,\n",
      "Nearest to first:  was, railways, hit, sons, wealth, chance, stamp, is,\n",
      "Nearest to used:  viral, denote, help, aligned, wavelength, al, hospital, scientific,\n",
      "Nearest to to:  write, the, only, finland, web, chamber, decision, compare,\n",
      "Nearest to often:  newer, hegel, pace, justify, ottawa, weak, missiles, abolished,\n",
      "Nearest to that:  the, line, offspring, in, so, card, copies, also,\n",
      "Nearest to two:  one, seven, eight, zero, four, three, nine, expenditures,\n",
      "Iteration 41000, loss = 0.760276913643\n",
      "Iteration 42000, loss = 0.776224255562\n",
      "Iteration 43000, loss = 0.645233869553\n",
      "Iteration 44000, loss = 0.656665325165\n",
      "Iteration 45000, loss = 0.678250491619\n",
      "Iteration 46000, loss = 0.735855102539\n",
      "Iteration 47000, loss = 0.637298226357\n",
      "Iteration 48000, loss = 0.703498005867\n",
      "Iteration 49000, loss = 0.682957291603\n",
      "Iteration 50000, loss = 0.695131838322\n",
      "Nearest to from:  the, of, and, nine, with, five, two, four,\n",
      "Nearest to no:  means, voting, washington, longer, denmark, mothers, universe, biography,\n",
      "Nearest to so:  river, diplomacy, locomotive, promised, individually, teaches, conrad, these,\n",
      "Nearest to of:  the, nine, a, in, one, is, and, two,\n",
      "Nearest to however:  universities, openly, finding, absorbed, four, the, six, of,\n",
      "Nearest to at:  time, stake, the, with, affairs, and, collection, on,\n",
      "Nearest to had:  been, the, a, to, was, discourse, five, as,\n",
      "Nearest to use:  phrase, context, extends, scientists, muslims, its, knights, chose,\n",
      "Nearest to i:  devil, victim, expressions, jerome, your, works, eiffel, painful,\n",
      "Nearest to are:  labels, contain, warsaw, beyond, resistant, verbs, perceived, in,\n",
      "Nearest to first:  observed, was, sons, cabinet, represent, stamp, characters, plot,\n",
      "Nearest to used:  months, milton, segments, han, viral, cologne, reflected, denote,\n",
      "Nearest to to:  the, in, of, was, a, six, jewish, an,\n",
      "Nearest to often:  nuclear, been, rand, computers, disorders, detroit, types, newer,\n",
      "Nearest to that:  of, in, a, indicates, an, is, applied, been,\n",
      "Nearest to two:  four, zero, nine, eight, one, three, seven, six,\n",
      "Iteration 51000, loss = 0.708060801029\n",
      "Iteration 52000, loss = 0.647129297256\n",
      "Iteration 53000, loss = 0.518268823624\n",
      "Iteration 54000, loss = 0.709551870823\n",
      "Iteration 55000, loss = 0.678587853909\n",
      "Iteration 56000, loss = 0.745027422905\n",
      "Iteration 57000, loss = 0.632161140442\n",
      "Iteration 58000, loss = 0.803778469563\n",
      "Iteration 59000, loss = 0.70554035902\n",
      "Iteration 60000, loss = 0.752870738506\n",
      "Nearest to from:  the, party, of, nine, and, by, in, land,\n",
      "Nearest to no:  voting, politician, washington, raf, steve, proof, discussing, friendship,\n",
      "Nearest to so:  october, anglo, nazis, promised, impose, had, bc, fort,\n",
      "Nearest to of:  the, in, a, is, nine, one, for, was,\n",
      "Nearest to however:  four, the, and, in, game, hood, euro, openly,\n",
      "Nearest to at:  the, and, stake, of, winning, a, line, for,\n",
      "Nearest to had:  a, the, been, with, and, to, which, was,\n",
      "Nearest to use:  knights, phrase, chose, summer, sources, which, eventually, context,\n",
      "Nearest to i:  victim, painful, your, devil, expressions, creole, winters, ma,\n",
      "Nearest to are:  and, objective, various, involved, in, or, of, labels,\n",
      "Nearest to first:  relate, down, railways, described, sons, version, plot, quantities,\n",
      "Nearest to used:  set, is, scientific, han, milton, expressed, returned, years,\n",
      "Nearest to to:  the, in, of, which, six, was, an, or,\n",
      "Nearest to often:  something, types, geometry, newer, toronto, la, hands, arises,\n",
      "Nearest to that:  the, in, of, would, offspring, a, be, field,\n",
      "Nearest to two:  four, zero, one, eight, seven, three, nine, six,\n",
      "Iteration 61000, loss = 0.629039406776\n",
      "Iteration 62000, loss = 0.715365350246\n",
      "Iteration 63000, loss = 0.64631485939\n",
      "Iteration 64000, loss = 0.727265655994\n",
      "Iteration 65000, loss = 0.253704428673\n",
      "Iteration 66000, loss = 0.727998316288\n",
      "Iteration 67000, loss = 0.550481915474\n",
      "Iteration 68000, loss = 0.403727680445\n",
      "Iteration 69000, loss = 0.617321431637\n",
      "Iteration 70000, loss = 0.570453107357\n",
      "Nearest to from:  party, the, nine, of, s, land, by, in,\n",
      "Nearest to no:  denmark, commercial, whatever, biography, attend, ruby, voting, politician,\n",
      "Nearest to so:  review, priest, manuscript, had, with, nazis, local, river,\n",
      "Nearest to of:  the, a, in, is, one, party, nine, and,\n",
      "Nearest to however:  in, four, the, his, nine, six, a, and,\n",
      "Nearest to at:  the, is, and, of, second, poles, a, he,\n",
      "Nearest to had:  a, the, been, of, to, which, five, with,\n",
      "Nearest to use:  specifically, which, the, of, with, languages, s, and,\n",
      "Nearest to i:  expensive, victim, trend, bag, devil, imprisoned, jerome, peru,\n",
      "Nearest to are:  with, or, a, the, which, step, language, and,\n",
      "Nearest to first:  was, which, at, described, characters, it, the, of,\n",
      "Nearest to used:  is, water, acting, expressed, years, sean, conversion, segments,\n",
      "Nearest to to:  the, in, was, a, as, had, s, an,\n",
      "Nearest to often:  however, contemporaries, geometry, arises, seen, types, destroy, later,\n",
      "Nearest to that:  in, of, the, a, which, be, to, an,\n",
      "Nearest to two:  four, eight, one, nine, zero, three, six, seven,\n",
      "Iteration 71000, loss = 0.721854627132\n",
      "Iteration 72000, loss = 0.911059498787\n",
      "Iteration 73000, loss = 0.554167866707\n",
      "Iteration 74000, loss = 0.607793033123\n",
      "Iteration 75000, loss = 0.566086173058\n",
      "Iteration 76000, loss = 0.709487915039\n",
      "Iteration 77000, loss = 0.553768873215\n",
      "Iteration 78000, loss = 0.434033215046\n",
      "Iteration 79000, loss = 0.610845148563\n",
      "Iteration 80000, loss = 0.352030813694\n",
      "Nearest to from:  party, the, of, by, it, a, nine, to,\n",
      "Nearest to no:  commercial, biography, denmark, discourse, means, aol, association, raf,\n",
      "Nearest to so:  october, local, constantinople, are, as, d, of, il,\n",
      "Nearest to of:  the, is, in, and, a, by, which, was,\n",
      "Nearest to however:  the, is, four, nine, three, six, eight, in,\n",
      "Nearest to at:  the, of, is, poles, time, point, be, in,\n",
      "Nearest to had:  a, the, been, and, also, of, to, by,\n",
      "Nearest to use:  the, of, which, is, in, u, languages, s,\n",
      "Nearest to i:  boeing, which, perfect, athenian, peru, trend, vessel, changes,\n",
      "Nearest to are:  which, or, the, any, and, to, time, is,\n",
      "Nearest to first:  was, of, is, it, use, the, which, and,\n",
      "Nearest to used:  is, critical, segments, wanted, been, acting, move, creed,\n",
      "Nearest to to:  the, in, was, as, an, s, a, it,\n",
      "Nearest to often:  seen, rand, geometry, education, object, predecessor, something, but,\n",
      "Nearest to that:  in, of, be, a, it, is, the, to,\n",
      "Nearest to two:  zero, eight, four, one, nine, three, six, seven,\n",
      "Iteration 81000, loss = 0.535737931728\n",
      "Iteration 82000, loss = 0.778241872787\n",
      "Iteration 83000, loss = 0.899279117584\n",
      "Iteration 84000, loss = 0.87503015995\n",
      "Iteration 85000, loss = 0.680094838142\n",
      "Iteration 86000, loss = 0.0223193094134\n",
      "Iteration 87000, loss = 0.0173923932016\n",
      "Iteration 88000, loss = 0.526196360588\n",
      "Iteration 89000, loss = 2.83207201958\n",
      "Iteration 90000, loss = 1.27709329128\n",
      "Nearest to from:  party, it, the, is, by, of, that, a,\n",
      "Nearest to no:  commercial, been, venture, ivy, denmark, these, a, close,\n",
      "Nearest to so:  that, october, science, priest, islands, turning, for, using,\n",
      "Nearest to of:  the, in, is, by, a, was, one, which,\n",
      "Nearest to however:  the, is, four, his, in, that, zero, three,\n",
      "Nearest to at:  with, the, is, a, time, of, and, he,\n",
      "Nearest to had:  a, the, been, by, and, five, of, on,\n",
      "Nearest to use:  of, languages, and, would, is, first, the, which,\n",
      "Nearest to i:  a, your, integral, vessel, precision, bacterial, facts, perfect,\n",
      "Nearest to are:  that, the, more, or, is, of, for, be,\n",
      "Nearest to first:  is, of, was, in, would, that, the, which,\n",
      "Nearest to used:  is, capitalist, the, stretched, reinforced, which, be, from,\n",
      "Nearest to to:  in, the, it, of, that, is, for, was,\n",
      "Nearest to often:  in, sequence, but, is, the, after, was, first,\n",
      "Nearest to that:  in, a, of, the, would, was, is, to,\n",
      "Nearest to two:  zero, four, eight, nine, three, one, seven, six,\n",
      "Iteration 91000, loss = 0.639423847198\n",
      "Iteration 92000, loss = 0.955422520638\n",
      "Iteration 93000, loss = 0.998843312263\n",
      "Iteration 94000, loss = 1.21594239317e-05\n",
      "Iteration 95000, loss = 0.807127058506\n",
      "Iteration 96000, loss = 0.000239818255068\n",
      "Iteration 97000, loss = 2.81238603592\n",
      "Iteration 98000, loss = 1.09162139893\n",
      "Iteration 99000, loss = 0.558589100838\n"
     ]
    }
   ],
   "source": [
    "# Now we can train this thing.\n",
    "arr_1 = np.zeros((1,))\n",
    "arr_2 = np.zeros((1,))\n",
    "arr_3 = np.zeros((1,))\n",
    "for cnt in range(epochs):\n",
    "    idx = np.random.randint(0, len(labels)-1)\n",
    "    arr_1[0,] = word_target[idx]\n",
    "    arr_2[0,] = word_context[idx]\n",
    "    arr_3[0,] = labels[idx]\n",
    "    loss = model.train_on_batch([arr_1, arr_2], arr_3)\n",
    "    if cnt % 1000 == 0:\n",
    "        print('Iteration {0}, loss = {1}'.format(cnt, loss))\n",
    "    if cnt % 10000 == 0:\n",
    "        sim_cb.run_sim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['six', 'eight', 'one', 'two', 'nine', 'seven']\n",
      "[22, 13, 4, 10, 9, 23]\n",
      "Nearest to six:  three, eight, one, nine, four, five, zero, and,\n",
      "Nearest to eight:  seven, nine, four, zero, three, one, two, six,\n",
      "Nearest to one:  nine, four, five, three, eight, zero, six, seven,\n",
      "Nearest to two:  four, nine, zero, eight, seven, three, one, and,\n",
      "Nearest to nine:  eight, seven, four, one, two, zero, six, three,\n",
      "Nearest to seven:  eight, four, nine, one, three, two, zero, five,\n"
     ]
    }
   ],
   "source": [
    "predict_on_validation_model(word_list=['six', 'eight', 'one', 'two', 'nine', 'seven'], dictionary=dictionary, reversed_dictionary=reversed_dictionary, validation_model=validation_model, vocab_size=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['acquire']\n",
      "[6791]\n",
      "Nearest to acquire:  able, paint, synonym, dry, active, karaoke, oath, skull,\n"
     ]
    }
   ],
   "source": [
    "predict_on_validation_model(word_list=['acquire'], dictionary=dictionary, reversed_dictionary=reversed_dictionary, validation_model=validation_model, vocab_size=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('writings', 1532),\n",
       " ('homomorphism', 9648),\n",
       " ('yellow', 2457),\n",
       " ('four', 21),\n",
       " ('prefix', 5771),\n",
       " ('jihad', 5607),\n",
       " ('hanging', 8021),\n",
       " ('cyprus', 3421),\n",
       " ('aggression', 7861),\n",
       " ('looking', 2399)]"
      ]
     },
     "execution_count": 613,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reversed_dictionary.items()[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['homomorphism', 'yellow', 'jihad']\n",
      "[9648, 2457, 5607]\n",
      "Nearest to homomorphism:  relationship, july, homogeneous, piece, nineteenth, squares, necessity, attacked,\n",
      "Nearest to yellow:  kingdom, interpretations, alchemy, functional, the, constituent, null, finish,\n",
      "Nearest to jihad:  ray, on, death, international, conditional, further, following, at,\n"
     ]
    }
   ],
   "source": [
    "predict_on_validation_model(word_list=['homomorphism', 'yellow', 'jihad'], dictionary=dictionary, reversed_dictionary=reversed_dictionary, validation_model=validation_model, vocab_size=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interesting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:basic_ml]",
   "language": "python",
   "name": "conda-env-basic_ml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
